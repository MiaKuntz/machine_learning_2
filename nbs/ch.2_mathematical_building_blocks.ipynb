{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-KYbuKm-efK"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mkrnMny-efM"
      },
      "source": [
        "# The mathematical building blocks of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgjpcKEs-efM"
      },
      "source": [
        "## A first look at a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qAw18F-efM"
      },
      "source": [
        "**Loading the MNIST dataset in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NdM0fXKW-efM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist # importing the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() # loading the dataset into training and testing data and labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lGvUzH1P-efN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape # displaying the shape of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eSOR2Sao-efN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_labels) # displaying the length of the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0ie4lQZD-efN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels # displaying the training labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iJ98VSuU-efN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_images.shape # displaying the shape of the testing images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4zn1w09E-efN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_labels) # displaying the length of the testing labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rvjsyAmv-efO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels # displaying the testing labels "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bU68xd-efO"
      },
      "source": [
        "**The network architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JhV_naNE-efO"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing the keras module from tensorflow\n",
        "from tensorflow.keras import layers # importing the layers module from keras \n",
        "model = keras.Sequential([ # creating a sequential model\n",
        "    layers.Dense(512, activation=\"relu\"), # adding a dense layer with 512 neurons and relu activation function\n",
        "    layers.Dense(10, activation=\"softmax\") # adding a dense layer with 10 neurons and softmax activation function\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq8Nk7Ha-efO"
      },
      "source": [
        "**The compilation step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BvfhUssD-efO"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the rmsprop optimizer \n",
        "              loss=\"sparse_categorical_crossentropy\", # using the sparse categorical crossentropy loss function \n",
        "              metrics=[\"accuracy\"]) # using the accuracy metric to evaluate the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ROj0rUg-efO"
      },
      "source": [
        "**Preparing the image data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MUuW1IDV-efO"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28)) # reshaping the training images to have 60000 rows and 28*28 columns \n",
        "train_images = train_images.astype(\"float32\") / 255 # normalizing the training images by dividing by 255 to scale the values between 0 and 1 \n",
        "test_images = test_images.reshape((10000, 28 * 28)) # reshaping the testing images to have 10000 rows and 28*28 columns \n",
        "test_images = test_images.astype(\"float32\") / 255 # normalizing the testing images by dividing by 255 to scale the values between 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs4hB8JZ-efO"
      },
      "source": [
        "**\"Fitting\" the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z-QqtWqS-efP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.2560 - accuracy: 0.9258\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1041 - accuracy: 0.9684\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.9795\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0507 - accuracy: 0.9848\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0379 - accuracy: 0.9886\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x17969df50>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128) # fitting the model to the training data with 5 epochs and a batch size of 128 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WwtEjbz-efP"
      },
      "source": [
        "**Using the model to make predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AF2bSAx5-efP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([1.0830179e-09, 6.0928373e-11, 3.8332070e-07, 1.2775445e-06,\n",
              "       6.1016237e-13, 7.6700601e-09, 3.5960821e-14, 9.9999833e-01,\n",
              "       2.7385409e-08, 2.0463677e-08], dtype=float32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_digits = test_images[0:10] # selecting the first 10 testing images \n",
        "predictions = model.predict(test_digits) # making predictions on the first 10 testing images \n",
        "predictions[0] # displaying the first prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5RjMpFld-efP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0].argmax() # displaying the index of the maximum value in the first prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G68JStoL-efP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.99999833"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[0][7] # displaying the probability of the first prediction being a 7 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2z5OySxD-efP"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels[0] # displaying the actual label of the first testing image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9QmM_Ms-efP"
      },
      "source": [
        "**Evaluating the model on new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-RE58wgf-efP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 445us/step - loss: 0.0642 - accuracy: 0.9805\n",
            "test_acc: 0.9804999828338623\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels) # evaluating the model on the testing data \n",
        "print(f\"test_acc: {test_acc}\") # printing the accuracy of the model on the testing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ3yI1jK-efP"
      },
      "source": [
        "## Data representations for neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laJqHkXK-efQ"
      },
      "source": [
        "### Scalars (rank-0 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BzTm-dDt-efQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np # importing the numpy module\n",
        "x = np.array(12) # creating a scalar \n",
        "x # displaying the scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5_86LgS9-efQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.ndim # displaying the number of dimensions of the scalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nDWzvxb-efQ"
      },
      "source": [
        "### Vectors (rank-1 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aG9US7XX-efQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14,  7])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([12, 3, 6, 14, 7]) # creating a 1D array \n",
        "x # displaying the 1D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UsZbdFE4-efQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.ndim # displaying the number of dimensions of the 1D array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z36SEXU-efQ"
      },
      "source": [
        "### Matrices (rank-2 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rf6QJYcO-efR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]]) # creating a 2D array\n",
        "x.ndim # displaying the number of dimensions of the 2D array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3xPiDua-efR"
      },
      "source": [
        "### Rank-3 and higher-rank tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ebf-tQt5-efR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]]) # creating a 3D array\n",
        "x.ndim # displaying the number of dimensions of the 3D array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifon0pck-efS"
      },
      "source": [
        "### Key attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OghU7iPJ-efS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist # importing the mnist dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() # loading the mnist dataset into training and testing data and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9fa0k7fA-efS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.ndim # displaying the number of dimensions of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GfvGY5Zi-efT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape # displaying the shape of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "l5eGU4e9-efT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.dtype # displaying the data type of the training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1scz-mUl-efT"
      },
      "source": [
        "**Displaying the fourth digit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zSnEBSG5-efT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/miakuntz/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"orientation\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/miakuntz/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"facecolor\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/miakuntz/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"edgecolor\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/miakuntz/Library/Python/3.11/lib/python/site-packages/IPython/core/pylabtools.py:152: MatplotlibDeprecationWarning: savefig() got unexpected keyword argument \"bbox_inches_restore\" which is no longer supported as of 3.3 and will become an error two minor releases later\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt # importing the matplotlib module\n",
        "digit = train_images[4] # selecting the 5th training image\n",
        "plt.imshow(digit, cmap=plt.cm.binary) # displaying the 5th training image\n",
        "plt.show() # displaying the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "utZZMdlz-efU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[4] # displaying the label of the 5th training image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKNUvjmU-efU"
      },
      "source": [
        "### Manipulating tensors in NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rAfot1vb-efU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_slice = train_images[10:100] # selecting a slice of the training images\n",
        "my_slice.shape # displaying the shape of the slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0ZydetJq-efU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_slice = train_images[10:100, :, :] # selecting a slice of the training images\n",
        "my_slice.shape # displaying the shape of the slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "rEvDOcmn-efU"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28] # selecting a slice of the training images\n",
        "my_slice.shape # displaying the shape of the slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "yivLNhjv-efU"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[:, 14:, 14:] # selecting a slice of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mRR33kzQ-efU"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[:, 7:-7, 7:-7] # selecting a slice of the training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jynXBK-S-efU"
      },
      "source": [
        "### The notion of data batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DPpO7o6y-efU"
      },
      "outputs": [],
      "source": [
        "batch = train_images[:128] # selecting a batch of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BTd76wTc-efU"
      },
      "outputs": [],
      "source": [
        "batch = train_images[128:256] # selecting a batch of the training images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xA1AuMdx-efV"
      },
      "outputs": [],
      "source": [
        "n = 3 # setting the batch size\n",
        "batch = train_images[128 * n:128 * (n + 1)] # selecting a batch of the training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWpTDwP_-efV"
      },
      "source": [
        "### Real-world examples of data tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldMPzdFv-efV"
      },
      "source": [
        "### Vector data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-xMHSai-efV"
      },
      "source": [
        "### Timeseries data or sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3-ljWqj-efV"
      },
      "source": [
        "### Image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4yAvsNU-efV"
      },
      "source": [
        "### Video data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L41kk1Ml-efV"
      },
      "source": [
        "## The gears of neural networks: tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz37O5BH-efV"
      },
      "source": [
        "### Element-wise operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "U6fb0Ja0-efV"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x): # defining a function to apply the relu function to an array\n",
        "    assert len(x.shape) == 2 # checking that the input is a 2D array\n",
        "    x = x.copy() # creating a copy of the input array\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the array\n",
        "        for j in range(x.shape[1]): # iterating over the columns of the array\n",
        "            x[i, j] = max(x[i, j], 0) # applying the relu function to each element of the array\n",
        "    return x # returning the modified array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pu2DwkcH-efV"
      },
      "outputs": [],
      "source": [
        "def naive_add(x, y): # defining a function to add two arrays\n",
        "    assert len(x.shape) == 2 # checking that the first input is a 2D array\n",
        "    assert x.shape == y.shape # checking that the two inputs have the same shape\n",
        "    x = x.copy() # creating a copy of the first input array\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the first input array\n",
        "        for j in range(x.shape[1]): # iterating over the columns of the first input array\n",
        "            x[i, j] += y[i, j] # adding the corresponding elements of the two input arrays\n",
        "    return x # returning the modified array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wi-6twwt-efV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took: 0.00 s\n"
          ]
        }
      ],
      "source": [
        "import time # importing the time module\n",
        "\n",
        "x = np.random.random((20, 100)) # creating a random 2D array \n",
        "y = np.random.random((20, 100)) # creating a random 2D array\n",
        "\n",
        "t0 = time.time() # recording the current time \n",
        "for _ in range(1000): # iterating 1000 times\n",
        "    z = x + y # adding the two arrays\n",
        "    z = np.maximum(z, 0.) # applying the relu function to the result\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0)) # printing the time taken to complete the operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1fg4pZQt-efV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Took: 0.70 s\n"
          ]
        }
      ],
      "source": [
        "t0 = time.time() # recording the current time\n",
        "for _ in range(1000): # iterating 1000 times\n",
        "    z = naive_add(x, y) # adding the two arrays\n",
        "    z = naive_relu(z) # applying the relu function to the result\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0)) # printing the time taken to complete the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsIFHLPT-efV"
      },
      "source": [
        "### Broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Fm4SEs5I-efV"
      },
      "outputs": [],
      "source": [
        "import numpy as np # importing the numpy module\n",
        "X = np.random.random((32, 10)) # creating a random 2D array\n",
        "y = np.random.random((10,)) # creating a random 1D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "GHxV3Mw0-efW"
      },
      "outputs": [],
      "source": [
        "y = np.expand_dims(y, axis=0) # expanding the dimensions of the 1D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-xKw1RX0-efW"
      },
      "outputs": [],
      "source": [
        "Y = np.concatenate([y] * 32, axis=0) # concatenating the 1D array 32 times along the first axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "C73jttbc-efW"
      },
      "outputs": [],
      "source": [
        "def naive_add_matrix_and_vector(x, y): # defining a function to add a matrix and a vector\n",
        "    assert len(x.shape) == 2 # checking that the first input is a 2D array\n",
        "    assert len(y.shape) == 1 # checking that the second input is a 1D array\n",
        "    assert x.shape[1] == y.shape[0] # checking that the two inputs have compatible shapes\n",
        "    x = x.copy() # creating a copy of the first input array\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the first input array\n",
        "        for j in range(x.shape[1]): # iterating over the columns of the first input array\n",
        "            x[i, j] += y[j] # adding the corresponding elements of the two input arrays\n",
        "    return x # returning the modified array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Do1reGPb-efW"
      },
      "outputs": [],
      "source": [
        "import numpy as np # importing the numpy module\n",
        "x = np.random.random((64, 3, 32, 10)) # creating a random 4D array\n",
        "y = np.random.random((32, 10)) # creating a random 2D array\n",
        "z = np.maximum(x, y) # applying the element-wise maximum function to the two arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9_S0rtc-efW"
      },
      "source": [
        "### Tensor product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HIPUPVnP-efW"
      },
      "outputs": [],
      "source": [
        "x = np.random.random((32,)) # creating a random 1D array\n",
        "y = np.random.random((32,)) # creating a random 1D array\n",
        "z = np.dot(x, y) # computing the dot product of the two arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qmIXzcQQ-efW"
      },
      "outputs": [],
      "source": [
        "def naive_vector_dot(x, y): # defining a function to compute the dot product of two arrays\n",
        "    assert len(x.shape) == 1 # checking that the first input is a 1D array\n",
        "    assert len(y.shape) == 1 # checking that the second input is a 1D array\n",
        "    assert x.shape[0] == y.shape[0] # checking that the two inputs have the same length\n",
        "    z = 0. # initializing the result\n",
        "    for i in range(x.shape[0]): # iterating over the elements of the first input array\n",
        "        z += x[i] * y[i] # computing the dot product of the two arrays\n",
        "    return z # returning the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wJQo13uB-efW"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y): # defining a function to compute the dot product of a matrix and a vector\n",
        "    assert len(x.shape) == 2 # checking that the first input is a 2D array\n",
        "    assert len(y.shape) == 1 # checking that the second input is a 1D array\n",
        "    assert x.shape[1] == y.shape[0] # checking that the two inputs have compatible shapes\n",
        "    z = np.zeros(x.shape[0]) # initializing the result\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the first input array\n",
        "        for j in range(x.shape[1]): # iterating over the columns of the first input array\n",
        "            z[i] += x[i, j] * y[j] # computing the dot product of the two arrays\n",
        "    return z # returning the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "gQk1rMlf-efW"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y): # defining a function to compute the dot product of a matrix and a vector\n",
        "    z = np.zeros(x.shape[0]) # initializing the result\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the first input array\n",
        "        z[i] = naive_vector_dot(x[i, :], y) # computing the dot product of the two arrays\n",
        "    return z # returning the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "II2Hiteg-efW"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_dot(x, y): # defining a function to compute the dot product of two matrices\n",
        "    assert len(x.shape) == 2 # checking that the first input is a 2D array\n",
        "    assert len(y.shape) == 2 # checking that the second input is a 2D array\n",
        "    assert x.shape[1] == y.shape[0] # checking that the two inputs have compatible shapes\n",
        "    z = np.zeros((x.shape[0], y.shape[1])) # initializing the result\n",
        "    for i in range(x.shape[0]): # iterating over the rows of the first input array\n",
        "        for j in range(y.shape[1]): # iterating over the columns of the second input array\n",
        "            row_x = x[i, :] # selecting the i-th row of the first input array\n",
        "            column_y = y[:, j] # selecting the j-th column of the second input array\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y) # computing the dot product of the two arrays\n",
        "    return z # returning the result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poks3DB0-efW"
      },
      "source": [
        "### Tensor reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "wmPSTwDG-efW"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28)) # reshaping the training images to have 60000 rows and 28*28 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IISOGE8c-efW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[0., 1.],\n",
        "             [2., 3.],\n",
        "             [4., 5.]]) # creating a 2D array\n",
        "x.shape # displaying the shape of the 2D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aAzlamph-efW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = x.reshape((6, 1)) # reshaping the 2D array to have 6 rows and 1 column\n",
        "x # displaying the reshaped 2D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "iK3r90mi-efX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 300)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.zeros((300, 20)) # creating a 2D array of zeros\n",
        "x = np.transpose(x) # transposing the 2D array\n",
        "x.shape # displaying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlL4i4lL-efX"
      },
      "source": [
        "### Geometric interpretation of tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECCtKYdY-efX"
      },
      "source": [
        "### A geometric interpretation of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xurhmClD-efX"
      },
      "source": [
        "## The engine of neural networks: gradient-based optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w--O79st-efX"
      },
      "source": [
        "### What's a derivative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dYXzun_-efX"
      },
      "source": [
        "### Derivative of a tensor operation: the gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCZMd6xQ-efX"
      },
      "source": [
        "### Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FdEd4R-efX"
      },
      "source": [
        "### Chaining derivatives: The Backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lubp0bAO-efX"
      },
      "source": [
        "#### The chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6RzMcOP-efX"
      },
      "source": [
        "#### Automatic differentiation with computation graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apoyjAcL-efX"
      },
      "source": [
        "#### The gradient tape in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "z19sjmNA-efX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the tensorflow module\n",
        "x = tf.Variable(0.) # creating a variable with a scalar value\n",
        "with tf.GradientTape() as tape: # opening a gradient tape\n",
        "    y = 2 * x + 3 # defining a function of the variable\n",
        "grad_of_y_wrt_x = tape.gradient(y, x) # computing the gradient of the function with respect to the variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dwa64oXd-efX"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2))) # creating a variable with a 2D array of random values\n",
        "with tf.GradientTape() as tape: # opening a gradient tape\n",
        "    y = 2 * x + 3 # defining a function of the variable\n",
        "grad_of_y_wrt_x = tape.gradient(y, x) # computing the gradient of the function with respect to the variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vYwrIFNz-efX"
      },
      "outputs": [],
      "source": [
        "W = tf.Variable(tf.random.uniform((2, 2))) # creating a variable with a 2D array of random values\n",
        "b = tf.Variable(tf.zeros((2,))) # creating a variable with a 1D array of zeros\n",
        "x = tf.random.uniform((2, 2)) # creating a 2D array of random values\n",
        "with tf.GradientTape() as tape: # opening a gradient tape\n",
        "    y = tf.matmul(x, W) + b # defining a function of the variable\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b]) # computing the gradient of the function with respect to the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOrt2ySX-efX"
      },
      "source": [
        "## Looking back at our first example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "S1xO40O3-efX"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() # loading the mnist dataset into training and testing data and labels\n",
        "train_images = train_images.reshape((60000, 28 * 28)) # reshaping the training images to have 60000 rows and 28*28 columns\n",
        "train_images = train_images.astype(\"float32\") / 255 # normalizing the training images by dividing by 255 to scale the values between 0 and 1\n",
        "test_images = test_images.reshape((10000, 28 * 28)) # reshaping the testing images to have 10000 rows and 28*28 columns\n",
        "test_images = test_images.astype(\"float32\") / 255 # normalizing the testing images by dividing by 255 to scale the values between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "4BG_TItS-efX"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([ # creating a sequential model\n",
        "    layers.Dense(512, activation=\"relu\"), # adding a dense layer with 512 neurons and relu activation function\n",
        "    layers.Dense(10, activation=\"softmax\") # adding a dense layer with 10 neurons and softmax activation function\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "cWRGt2ZP-efX"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the rmsprop optimizer\n",
        "              loss=\"sparse_categorical_crossentropy\", # using the sparse categorical crossentropy loss function\n",
        "              metrics=[\"accuracy\"]) # using the accuracy metric to evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "BsZqb8Fs-efY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 13/469 [..............................] - ETA: 1s - loss: 1.1206 - accuracy: 0.6731  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 1s 2ms/step - loss: 0.2543 - accuracy: 0.9260\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1048 - accuracy: 0.9691\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.9796\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.9847\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9886\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x17ed3f310>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128) # fitting the model to the training data with 5 epochs and a batch size of 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x76IWuV1-efY"
      },
      "source": [
        "### Reimplementing our first example from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ywiV_bo-efY"
      },
      "source": [
        "#### A simple Dense class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HiLhymyl-efZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the tensorflow module\n",
        "\n",
        "class NaiveDense: # defining a class to create a dense layer\n",
        "    def __init__(self, input_size, output_size, activation): # defining the constructor of the class\n",
        "        self.activation = activation # setting the activation function\n",
        "\n",
        "        w_shape = (input_size, output_size) # setting the shape of the weights\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1) # setting the initial value of the weights\n",
        "        self.W = tf.Variable(w_initial_value) # creating a variable for the weights\n",
        "\n",
        "        b_shape = (output_size,) # setting the shape of the biases\n",
        "        b_initial_value = tf.zeros(b_shape) # setting the initial value of the biases\n",
        "        self.b = tf.Variable(b_initial_value) # creating a variable for the biases\n",
        "\n",
        "    def __call__(self, inputs): # defining the call method of the class\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b) # returning the result of the dense layer\n",
        "\n",
        "    @property # defining a property to access the weights\n",
        "    def weights(self): # defining a method to access the weights\n",
        "        return [self.W, self.b] # returning the weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TsJVgqu-efZ"
      },
      "source": [
        "#### A simple Sequential class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ZTkEnOSi-efZ"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential: # defining a class to create a sequential model\n",
        "    def __init__(self, layers): # defining the constructor of the class\n",
        "        self.layers = layers # setting the layers of the model\n",
        "\n",
        "    def __call__(self, inputs): # defining the call method of the class\n",
        "        x = inputs # setting the input of the model\n",
        "        for layer in self.layers: # iterating over the layers of the model\n",
        "           x = layer(x) # applying the layer to the input\n",
        "        return x # returning the output of the model\n",
        "\n",
        "    @property # defining a property to access the weights\n",
        "    def weights(self): # defining a method to access the weights\n",
        "       weights = [] # initializing the weights\n",
        "       for layer in self.layers: # iterating over the layers of the model\n",
        "           weights += layer.weights # adding the weights of the layer to the weights\n",
        "       return weights # returning the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "1NqdeVIZ-efZ"
      },
      "outputs": [],
      "source": [
        "model = NaiveSequential([ # creating a naive sequential model\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu), # adding a naive dense layer with 512 neurons and relu activation function\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax) # adding a naive dense layer with 10 neurons and softmax activation function\n",
        "])\n",
        "assert len(model.weights) == 4 # checking that the model has 4 weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg6VQRfg-efZ"
      },
      "source": [
        "#### A batch generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "pLW0Wbnv-efZ"
      },
      "outputs": [],
      "source": [
        "import math # importing the math module\n",
        "\n",
        "class BatchGenerator: # defining a class to create a batch generator\n",
        "    def __init__(self, images, labels, batch_size=128): # defining the constructor of the class\n",
        "        assert len(images) == len(labels) # checking that the number of images and labels are the same\n",
        "        self.index = 0 # initializing the index\n",
        "        self.images = images # setting the images\n",
        "        self.labels = labels # setting the labels\n",
        "        self.batch_size = batch_size # setting the batch size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size) # computing the number of batches\n",
        "\n",
        "    def next(self): # defining a method to get the next batch\n",
        "        images = self.images[self.index : self.index + self.batch_size] # selecting the images of the next batch\n",
        "        labels = self.labels[self.index : self.index + self.batch_size] # selecting the labels of the next batch\n",
        "        self.index += self.batch_size # updating the index\n",
        "        return images, labels # returning the next batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vboy92cG-efZ"
      },
      "source": [
        "### Running one training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CtF9S038-efZ"
      },
      "outputs": [],
      "source": [
        "def one_training_step(model, images_batch, labels_batch): # defining a function to perform one training step\n",
        "    with tf.GradientTape() as tape: # opening a gradient tape\n",
        "        predictions = model(images_batch) # making predictions on the images batch\n",
        "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy( # computing the per sample losses\n",
        "            labels_batch, predictions) # using the sparse categorical crossentropy loss function\n",
        "        average_loss = tf.reduce_mean(per_sample_losses) # computing the average loss\n",
        "    gradients = tape.gradient(average_loss, model.weights) # computing the gradients of the average loss with respect to the weights\n",
        "    update_weights(gradients, model.weights) # updating the weights of the model\n",
        "    return average_loss # returning the average loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "m7bg0eQT-efZ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3 # setting the learning rate\n",
        "\n",
        "def update_weights(gradients, weights): # defining a function to update the weights\n",
        "    for g, w in zip(gradients, weights): # iterating over the gradients and weights\n",
        "        w.assign_sub(g * learning_rate) # updating the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "LP8Wvkt5-efZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import optimizers # importing the optimizers module from keras\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3) # creating an optimizer with the stochastic gradient descent algorithm and a learning rate of 1e-3\n",
        "\n",
        "def update_weights(gradients, weights): # defining a function to update the weights\n",
        "    optimizer.apply_gradients(zip(gradients, weights)) # updating the weights using the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRGo_Tgq-efZ"
      },
      "source": [
        "### The full training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_NDT4NIx-efZ"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128): # defining a function to fit a model to the data\n",
        "    for epoch_counter in range(epochs): # iterating over the epochs\n",
        "        print(f\"Epoch {epoch_counter}\") # printing the current epoch\n",
        "        batch_generator = BatchGenerator(images, labels) # creating a batch generator\n",
        "        for batch_counter in range(batch_generator.num_batches): # iterating over the batches\n",
        "            images_batch, labels_batch = batch_generator.next() # getting the next batch\n",
        "            loss = one_training_step(model, images_batch, labels_batch) # performing one training step\n",
        "            if batch_counter % 100 == 0: # checking if the current batch is a multiple of 100\n",
        "                print(f\"loss at batch {batch_counter}: {loss:.2f}\") # printing the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "BxXA897n-efZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at batch 0: 3.58\n",
            "loss at batch 100: 2.24\n",
            "loss at batch 200: 2.18\n",
            "loss at batch 300: 2.05\n",
            "loss at batch 400: 2.22\n",
            "Epoch 1\n",
            "loss at batch 0: 1.88\n",
            "loss at batch 100: 1.87\n",
            "loss at batch 200: 1.80\n",
            "loss at batch 300: 1.67\n",
            "loss at batch 400: 1.82\n",
            "Epoch 2\n",
            "loss at batch 0: 1.55\n",
            "loss at batch 100: 1.57\n",
            "loss at batch 200: 1.47\n",
            "loss at batch 300: 1.39\n",
            "loss at batch 400: 1.50\n",
            "Epoch 3\n",
            "loss at batch 0: 1.30\n",
            "loss at batch 100: 1.33\n",
            "loss at batch 200: 1.21\n",
            "loss at batch 300: 1.18\n",
            "loss at batch 400: 1.26\n",
            "Epoch 4\n",
            "loss at batch 0: 1.10\n",
            "loss at batch 100: 1.15\n",
            "loss at batch 200: 1.02\n",
            "loss at batch 300: 1.03\n",
            "loss at batch 400: 1.09\n",
            "Epoch 5\n",
            "loss at batch 0: 0.96\n",
            "loss at batch 100: 1.02\n",
            "loss at batch 200: 0.89\n",
            "loss at batch 300: 0.91\n",
            "loss at batch 400: 0.97\n",
            "Epoch 6\n",
            "loss at batch 0: 0.85\n",
            "loss at batch 100: 0.91\n",
            "loss at batch 200: 0.79\n",
            "loss at batch 300: 0.82\n",
            "loss at batch 400: 0.89\n",
            "Epoch 7\n",
            "loss at batch 0: 0.77\n",
            "loss at batch 100: 0.83\n",
            "loss at batch 200: 0.71\n",
            "loss at batch 300: 0.76\n",
            "loss at batch 400: 0.82\n",
            "Epoch 8\n",
            "loss at batch 0: 0.71\n",
            "loss at batch 100: 0.76\n",
            "loss at batch 200: 0.65\n",
            "loss at batch 300: 0.70\n",
            "loss at batch 400: 0.77\n",
            "Epoch 9\n",
            "loss at batch 0: 0.66\n",
            "loss at batch 100: 0.70\n",
            "loss at batch 200: 0.60\n",
            "loss at batch 300: 0.66\n",
            "loss at batch 400: 0.73\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist # importing the mnist dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() # loading the mnist dataset into training and testing data and labels\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28)) # reshaping the training images to have 60000 rows and 28*28 columns\n",
        "train_images = train_images.astype(\"float32\") / 255 # normalizing the training images by dividing by 255 to scale the values between 0 and 1\n",
        "test_images = test_images.reshape((10000, 28 * 28)) # reshaping the testing images to have 10000 rows and 28*28 columns\n",
        "test_images = test_images.astype(\"float32\") / 255 # normalizing the testing images by dividing by 255 to scale the values between 0 and 1\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128) # fitting the model to the training data with 10 epochs and a batch size of 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gyzS67k-efZ"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "7JcOsNxN-efZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.82\n"
          ]
        }
      ],
      "source": [
        "predictions = model(test_images) # making predictions on the testing images\n",
        "predictions = predictions.numpy() # converting the predictions to a numpy array\n",
        "predicted_labels = np.argmax(predictions, axis=1) # selecting the index of the maximum value in each prediction\n",
        "matches = predicted_labels == test_labels # checking if the predicted labels match the actual labels\n",
        "print(f\"accuracy: {matches.mean():.2f}\") # printing the accuracy of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjK50WZ-efa"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter02_mathematical-building-blocks.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
