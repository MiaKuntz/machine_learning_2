{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRvmC5Uc65j8"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcCa1xUG65j9"
      },
      "source": [
        "## Modern convnet architecture patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in_GzAAc65j9"
      },
      "source": [
        "### Modularity, hierarchy, and reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq0ZdLXh65j-"
      },
      "source": [
        "### Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57PIPoYA65j-"
      },
      "source": [
        "**Residual block where the number of filters changes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsQKlGWq65j_"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing keras from tensorflow \n",
        "from tensorflow.keras import layers # importing layers from tensorflow.keras\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3)) # specifying the input shape of the model as 32x32x3 because we are using CIFAR-10 dataset which has 32x32x3 images \n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs) # adding a convolutional layer with 32 filters of size 3x3 and relu activation function\n",
        "residual = x # saving the output of the convolutional layer in a variable called residual\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) # adding another convolutional layer with 64 filters of size 3x3 and relu activation function\n",
        "residual = layers.Conv2D(64, 1)(residual) # adding a convolutional layer with 64 filters of size 1x1 to the residual variable\n",
        "x = layers.add([x, residual]) # adding the output of the second convolutional layer and the residual variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT8ea5GI65j_"
      },
      "source": [
        "**Case where target block includes a max pooling layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKUzwt_C65kA"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3)) # specifying the input shape of the model as 32x32x3 because we are using CIFAR-10 dataset which has 32x32x3 images\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs) # adding a convolutional layer with 32 filters of size 3x3 and relu activation function\n",
        "residual = x # saving the output of the convolutional layer in a variable called residual\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x) # adding another convolutional layer with 64 filters of size 3x3 and relu activation function\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x) # adding a max pooling layer with a pool size of 2x2\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual) # adding a convolutional layer with 64 filters of size 1x1 and a stride of 2 to the residual variable\n",
        "x = layers.add([x, residual]) # adding the output of the max pooling layer and the residual variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RP8oDK__65kB"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3)) # specifying the input shape of the model as 32x32x3 because we are using CIFAR-10 dataset which has 32x32x3 images\n",
        "x = layers.Rescaling(1./255)(inputs) # rescaling the input data to be between 0 and 1\n",
        "\n",
        "def residual_block(x, filters, pooling=False): # defining a function called residual_block that takes in the input x, the number of filters and a pooling parameter\n",
        "    residual = x # saving the input in a variable called residual\n",
        "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x) # adding a convolutional layer with the specified number of filters, a filter size of 3x3, relu activation function and same padding\n",
        "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x) # adding another convolutional layer with the specified number of filters, a filter size of 3x3, relu activation function and same padding\n",
        "    if pooling: # if the pooling parameter is True\n",
        "        x = layers.MaxPooling2D(2, padding=\"same\")(x) # add a max pooling layer with a pool size of 2x2\n",
        "        residual = layers.Conv2D(filters, 1, strides=2)(residual) # add a convolutional layer with the specified number of filters, a filter size of 1x1 and a stride of 2 to the residual variable\n",
        "    elif filters != residual.shape[-1]: # if the number of filters is not equal to the number of channels in the residual variable\n",
        "        residual = layers.Conv2D(filters, 1)(residual) # add a convolutional layer with the specified number of filters and a filter size of 1x1 to the residual variable\n",
        "    x = layers.add([x, residual]) # add the output of the second convolutional layer and the residual variable\n",
        "    return x # returning the output\n",
        "\n",
        "x = residual_block(x, filters=32, pooling=True) # calling the residual_block function with the input x, 32 filters and pooling set to True\n",
        "x = residual_block(x, filters=64, pooling=True) # calling the residual_block function with the input x, 64 filters and pooling set to True\n",
        "x = residual_block(x, filters=128, pooling=False) # calling the residual_block function with the input x, 128 filters and pooling set to False\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x) # adding a global average pooling layer\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # adding a dense layer with 1 unit and a sigmoid activation function\n",
        "model = keras.Model(inputs=inputs, outputs=outputs) # creating a model with the specified inputs and outputs\n",
        "model.summary() # printing a summary of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQWsxwTF65kC"
      },
      "source": [
        "### Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfbYKTgP65kC"
      },
      "source": [
        "### Depthwise separable convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_58WQUUB65kD"
      },
      "source": [
        "### Putting it together: A mini Xception-like model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd8y5gRq65kD"
      },
      "outputs": [],
      "source": [
        "from google.colab import files # importing files from google.colab\n",
        "files.upload() # uploading the kaggle.json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6MqRua-65kD"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle # creating a directory called .kaggle\n",
        "!cp kaggle.json ~/.kaggle/ # copying the kaggle.json file to the .kaggle directory\n",
        "!chmod 600 ~/.kaggle/kaggle.json # changing the permissions of the kaggle.json file\n",
        "!kaggle competitions download -c dogs-vs-cats # downloading the dogs-vs-cats competition data\n",
        "!unzip -qq train.zip # unzipping the train.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgtO5gGg65kE"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib # importing os, shutil and pathlib\n",
        "from tensorflow.keras.utils import image_dataset_from_directory # importing image_dataset_from_directory from tensorflow.keras.utils\n",
        "\n",
        "original_dir = pathlib.Path(\"train\") # specifying the original directory where the data is stored\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\") # specifying the new base directory where the data will be stored\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index): # defining a function called make_subset that takes in the subset name, the start index and the end index\n",
        "    for category in (\"cat\", \"dog\"): # looping through the categories\n",
        "        dir = new_base_dir / subset_name / category # specifying the directory where the subset data will be stored\n",
        "        os.makedirs(dir) # creating the directory\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)] # creating a list of file names\n",
        "        for fname in fnames: # looping through the file names\n",
        "            shutil.copyfile(src=original_dir / fname, # copying the files from the original directory to the new directory\n",
        "                            dst=dir / fname) # specifying the source and destination paths\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000) # calling the make_subset function to create the training subset\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500) # calling the make_subset function to create the validation subset\n",
        "make_subset(\"test\", start_index=1500, end_index=2500) # calling the make_subset function to create the test subset\n",
        "\n",
        "train_dataset = image_dataset_from_directory( # creating a training dataset using the image_dataset_from_directory function\n",
        "    new_base_dir / \"train\", # specifying the directory where the training data is stored\n",
        "    image_size=(180, 180), # specifying the image size\n",
        "    batch_size=32) # specifying the batch size\n",
        "validation_dataset = image_dataset_from_directory( # creating a validation dataset using the image_dataset_from_directory function\n",
        "    new_base_dir / \"validation\", # specifying the directory where the validation data is stored\n",
        "    image_size=(180, 180), # specifying the image size\n",
        "    batch_size=32) # specifying the batch size\n",
        "test_dataset = image_dataset_from_directory( # creating a test dataset using the image_dataset_from_directory function\n",
        "    new_base_dir / \"test\", # specifying the directory where the test data is stored\n",
        "    image_size=(180, 180), # specifying the image size\n",
        "    batch_size=32) # specifying the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7fmBzap65kE"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential( # creating a data augmentation layer\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"), # adding a random flip layer\n",
        "        layers.RandomRotation(0.1), # adding a random rotation layer\n",
        "        layers.RandomZoom(0.2), # adding a random zoom layer\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6vY1c2g65kF"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3)) # specifying the input shape of the model as 180x180x3\n",
        "x = data_augmentation(inputs) # adding the data augmentation layer\n",
        "\n",
        "x = layers.Rescaling(1./255)(x) # rescaling the input data to be between 0 and 1\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x) # adding a convolutional layer with 32 filters of size 5x5 and no bias\n",
        "\n",
        "for size in [32, 64, 128, 256, 512]: # looping through the sizes\n",
        "    residual = x # saving the input in a variable called residual\n",
        "\n",
        "    x = layers.BatchNormalization()(x) # adding a batch normalization layer\n",
        "    x = layers.Activation(\"relu\")(x) # adding a relu activation layer\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x) # adding a separable convolutional layer with the specified number of filters, a filter size of 3x3, same padding and no bias\n",
        "\n",
        "    x = layers.BatchNormalization()(x) # adding a batch normalization layer\n",
        "    x = layers.Activation(\"relu\")(x) # adding a relu activation layer\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x) # adding another separable convolutional layer with the specified number of filters, a filter size of 3x3, same padding and no bias\n",
        "\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x) # adding a max pooling layer with a pool size of 3x3, a stride of 2 and same padding\n",
        "\n",
        "    residual = layers.Conv2D( # adding a convolutional layer to the residual variable\n",
        "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual) # with the specified number of filters, a filter size of 1x1, a stride of 2, same padding and no bias\n",
        "    x = layers.add([x, residual]) # adding the output of the max pooling layer and the residual variable\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x) # adding a global average pooling layer\n",
        "x = layers.Dropout(0.5)(x) # adding a dropout layer with a rate of 0.5\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # adding a dense layer with 1 unit and a sigmoid activation function\n",
        "model = keras.Model(inputs=inputs, outputs=outputs) # creating a model with the specified inputs and outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2rBh0lR65kG"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\", # compiling the model with binary crossentropy loss\n",
        "              optimizer=\"rmsprop\", # using the rmsprop optimizer\n",
        "              metrics=[\"accuracy\"]) # using accuracy as the metric\n",
        "history = model.fit( # training the model\n",
        "    train_dataset, # using the training dataset\n",
        "    epochs=100, # training for 100 epochs\n",
        "    validation_data=validation_dataset) # using the validation dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter09_part02_modern-convnet-architecture-patterns.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
