{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaF7wnT7HJJ"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEpSLAUJ7HJM"
      },
      "source": [
        "## Interpreting what convnets learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Fx25nY7HJM"
      },
      "source": [
        "### Visualizing intermediate activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pxyj7dS7HJM"
      },
      "outputs": [],
      "source": [
        "# You can use this to load the file \"convnet_from_scratch_with_augmentation.keras\" you obtained in the last chapter\n",
        "from google.colab import files # importing the files module from google colab\n",
        "files.upload() # uploading the file \"convnet_from_scratch_with_augmentation.keras\" to google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWHBcXOh7HJN"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing the keras module from tensorflow\n",
        "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\") # loading the model \"convnet_from_scratch_with_augmentation.keras\"\n",
        "model.summary() # printing the summary of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xitnjs1D7HJN"
      },
      "source": [
        "**Preprocessing a single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFrf4vkG7HJN"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing the keras module from tensorflow\n",
        "import numpy as np # importing the numpy module\n",
        "\n",
        "img_path = keras.utils.get_file( # getting the file \"cat.jpg\" from the url \"https://img-datasets.s3.amazonaws.com/cat.jpg\"\n",
        "    fname=\"cat.jpg\", # setting the file name to \"cat.jpg\"\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\") # setting the url to \"https://img-datasets.s3.amazonaws.com/cat.jpg\"\n",
        "\n",
        "def get_img_array(img_path, target_size): # defining a function \"get_img_array\" with parameters \"img_path\" and \"target_size\"\n",
        "    img = keras.utils.load_img( # loading the image from the path \"img_path\"\n",
        "        img_path, target_size=target_size) # setting the target size to \"target_size\"\n",
        "    array = keras.utils.img_to_array(img) # converting the image to an array\n",
        "    array = np.expand_dims(array, axis=0) # expanding the dimensions of the array\n",
        "    return array # returning the array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(180, 180)) # getting the image array with the target size of (180, 180) (180 pixels by 180 pixels, because the model was trained on images of this size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxL7U0tt7HJN"
      },
      "source": [
        "**Displaying the test picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-qr3Wxm7HJN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # importing the matplotlib.pyplot module\n",
        "plt.axis(\"off\") # turning off the axis\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\")) # displaying the image\n",
        "plt.show() # displaying the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNStQTld7HJN"
      },
      "source": [
        "**Instantiating a model that returns layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loY_EIYV7HJO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers # importing the layers module from tensorflow.keras\n",
        "\n",
        "layer_outputs = [] # creating an empty list \"layer_outputs\"\n",
        "layer_names = [] # creating an empty list \"layer_names\"\n",
        "for layer in model.layers: # iterating through the layers of the model\n",
        "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)): # checking if the layer is an instance of the Conv2D or MaxPooling2D class\n",
        "        layer_outputs.append(layer.output) # appending the output of the layer to the list \"layer_outputs\"\n",
        "        layer_names.append(layer.name) # appending the name of the layer to the list \"layer_names\"\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs) # creating a model with the input as the input of the model and the outputs as the list \"layer_outputs\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj65Pt0w7HJO"
      },
      "source": [
        "**Using the model to compute layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nP5xirL17HJO"
      },
      "outputs": [],
      "source": [
        "activations = activation_model.predict(img_tensor) # getting the activations of the model on the image tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRkBHrGd7HJO"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0] # getting the first layer activation\n",
        "print(first_layer_activation.shape) # printing the shape of the first layer activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSsMoFXj7HJO"
      },
      "source": [
        "**Visualizing the fifth channel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOx6pwH17HJO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # importing the matplotlib.pyplot module\n",
        "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\") # displaying the first layer activation of the 5th filter using the \"viridis\" colormap (a colormap that goes from yellow for low values to blue for high values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGPUkrCo7HJO"
      },
      "source": [
        "**Visualizing every channel in every intermediate activation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_uXH2ID7HJO"
      },
      "outputs": [],
      "source": [
        "images_per_row = 16 # setting the number of images per row to 16\n",
        "for layer_name, layer_activation in zip(layer_names, activations): # iterating through the layer names and activations\n",
        "    n_features = layer_activation.shape[-1] # getting the number of features\n",
        "    size = layer_activation.shape[1] # getting the size of the layer activation\n",
        "    n_cols = n_features // images_per_row # getting the number of columns\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1, \n",
        "                             images_per_row * (size + 1) - 1)) # creating a display grid with zeros of the size of the layer activation and the number of columns and rows calculated above (with a border of 1 pixel between the images) \n",
        "    for col in range(n_cols): # iterating through the columns\n",
        "        for row in range(images_per_row): # iterating through the rows\n",
        "            channel_index = col * images_per_row + row # calculating the channel index by multiplying the column by the number of images per row and adding the row index to it \n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy() # getting the channel image by copying the channel from the layer activation \n",
        "            if channel_image.sum() != 0: # checking if the sum of the channel image is not zero\n",
        "                channel_image -= channel_image.mean() # subtracting the mean of the channel image from the channel image\n",
        "                channel_image /= channel_image.std() # dividing the channel image by the standard deviation of the channel image\n",
        "                channel_image *= 64 # multiplying the channel image by 64\n",
        "                channel_image += 128 # adding 128 to the channel image\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\") # clipping the channel image to be between 0 and 255 and converting it to an unsigned integer\n",
        "            display_grid[ # setting the display grid to the channel image\n",
        "                col * (size + 1): (col + 1) * size + col, # setting the column index to the column multiplied by the size plus 1 and the column plus 1 multiplied by the size plus the column\n",
        "                row * (size + 1) : (row + 1) * size + row] = channel_image # setting the row index to the row multiplied by the size plus 1 and the row plus 1 multiplied by the size plus the row\n",
        "    scale = 1. / size # setting the scale to 1 divided by the size \n",
        "    plt.figure(figsize=(scale * display_grid.shape[1], # setting the figure size to the scale multiplied by the display grid shape\n",
        "                        scale * display_grid.shape[0])) # setting the figure size to the scale multiplied by the display grid shape \n",
        "    plt.title(layer_name) # setting the title of the plot to the layer name\n",
        "    plt.grid(False) # turning off the grid\n",
        "    plt.axis(\"off\") # turning off the axis\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\") # displaying the display grid with the \"viridis\" colormap and the aspect ratio set to \"auto\" (so that the images are not stretched) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBVzMnO37HJP"
      },
      "source": [
        "### Visualizing convnet filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxxr--wL7HJP"
      },
      "source": [
        "**Instantiating the Xception convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftleDpph7HJP"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception( # loading the Xception model\n",
        "    weights=\"imagenet\", # setting the weights to \"imagenet\" which means that the model is pretrained on the ImageNet dataset\n",
        "    include_top=False) # setting the include top to False which means that the top layer of the model is not included"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZZAcNiw7HJP"
      },
      "source": [
        "**Printing the names of all convolutional layers in Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8H2Enla7HJP"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers: # iterating through the layers of the model\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)): # checking if the layer is an instance of the Conv2D or SeparableConv2D class\n",
        "        print(layer.name) # printing the name of the layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpon36g_7HJP"
      },
      "source": [
        "**Creating a feature extractor model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdCCZsI-7HJP"
      },
      "outputs": [],
      "source": [
        "layer_name = \"block3_sepconv1\" # setting the layer name to \"block3_sepconv1\"\n",
        "layer = model.get_layer(name=layer_name) # getting the layer with the name \"layer_name\"\n",
        "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output) # creating a model with the input as the input of the model and the output as the output of the layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ncK98v_7HJP"
      },
      "source": [
        "**Using the feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8g5QdFR7HJP"
      },
      "outputs": [],
      "source": [
        "activation = feature_extractor( # getting the activation of the feature extractor on the image tensor\n",
        "    keras.applications.xception.preprocess_input(img_tensor) # preprocessing the image tensor using the preprocess input function of the Xception model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4pYZZq77HJP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the tensorflow module\n",
        "\n",
        "def compute_loss(image, filter_index): # defining a function \"compute_loss\" with parameters \"image\" and \"filter_index\"\n",
        "    activation = feature_extractor(image) # getting the activation of the feature extractor on the image\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index] # getting the filter activation of the activation\n",
        "    return tf.reduce_mean(filter_activation) # returning the mean of the filter activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbPGqkKZ7HJP"
      },
      "source": [
        "**Loss maximization via stochastic gradient ascent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8FQL_sL7HJQ"
      },
      "outputs": [],
      "source": [
        "@tf.function # compiling the function to a TensorFlow graph\n",
        "def gradient_ascent_step(image, filter_index, learning_rate): # defining a function \"gradient_ascent_step\" with parameters \"image\", \"filter_index\", and \"learning_rate\"\n",
        "    with tf.GradientTape() as tape: # creating a gradient tape\n",
        "        tape.watch(image) # watching the image\n",
        "        loss = compute_loss(image, filter_index) # computing the loss of the image and the filter index\n",
        "    grads = tape.gradient(loss, image) # getting the gradients of the loss with respect to the image\n",
        "    grads = tf.math.l2_normalize(grads) # normalizing the gradients\n",
        "    image += learning_rate * grads # adding the learning rate multiplied by the gradients to the image\n",
        "    return image # returning the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFtPfhob7HJQ"
      },
      "source": [
        "**Function to generate filter visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKrh8SWe7HJQ"
      },
      "outputs": [],
      "source": [
        "img_width = 200 # setting the image width to 200\n",
        "img_height = 200 # setting the image height to 200\n",
        "\n",
        "def generate_filter_pattern(filter_index): # defining a function \"generate_filter_pattern\" with the parameter \"filter_index\"\n",
        "    iterations = 30 # setting the number of iterations to 30\n",
        "    learning_rate = 10. # setting the learning rate to 10.\n",
        "    image = tf.random.uniform( # generating a random uniform image\n",
        "        minval=0.4, # setting the minimum value to 0.4\n",
        "        maxval=0.6, # setting the maximum value to 0.6\n",
        "        shape=(1, img_width, img_height, 3)) # setting the shape to (1, img_width, img_height, 3)\n",
        "    for i in range(iterations): # iterating through the range of iterations\n",
        "        image = gradient_ascent_step(image, filter_index, learning_rate) # getting the image after the gradient ascent step\n",
        "    return image[0].numpy() # returning the image as a numpy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97DkbONW7HJQ"
      },
      "source": [
        "**Utility function to convert a tensor into a valid image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okf2ndAL7HJQ"
      },
      "outputs": [],
      "source": [
        "def deprocess_image(image): # defining a function \"deprocess_image\" with the parameter \"image\"\n",
        "    image -= image.mean() # subtracting the mean of the image from the image\n",
        "    image /= image.std() # dividing the image by the standard deviation of the image\n",
        "    image *= 64 # multiplying the image by 64\n",
        "    image += 128 # adding 128 to the image\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\") # clipping the image to be between 0 and 255 and converting it to an unsigned integer\n",
        "    image = image[25:-25, 25:-25, :] # cropping the image\n",
        "    return image # returning the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXiFz0n67HJQ"
      },
      "outputs": [],
      "source": [
        "plt.axis(\"off\") # turning off the axis\n",
        "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2))) # displaying the deprocessed image of the generated filter pattern with the filter index of 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0_jSokv7HJR"
      },
      "source": [
        "**Generating a grid of all filter response patterns in a layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3ah4WT57HJR"
      },
      "outputs": [],
      "source": [
        "all_images = [] # creating an empty list \"all_images\" \n",
        "for filter_index in range(64): # iterating through the range of 64\n",
        "    print(f\"Processing filter {filter_index}\") # printing the filter index\n",
        "    image = deprocess_image( # deprocessing the image\n",
        "        generate_filter_pattern(filter_index) # generating the filter pattern with the filter index\n",
        "    )\n",
        "    all_images.append(image) # appending the image to the list \"all_images\"\n",
        "\n",
        "margin = 5 # setting the margin to 5\n",
        "n = 8 # setting the number of images per row to 8\n",
        "cropped_width = img_width - 25 * 2 # setting the cropped width to the image width minus 25 pixels on each side\n",
        "cropped_height = img_height - 25 * 2 # setting the cropped height to the image height minus 25 pixels on each side\n",
        "width = n * cropped_width + (n - 1) * margin # setting the width to the number of images per row multiplied by the cropped width plus the number of images per row minus 1 multiplied by the margin\n",
        "height = n * cropped_height + (n - 1) * margin # setting the height to the number of images per row multiplied by the cropped height plus the number of images per row minus 1 multiplied by the margin\n",
        "stitched_filters = np.zeros((width, height, 3)) # creating a matrix of zeros with the width, height, and 3 channels\n",
        "\n",
        "for i in range(n): # iterating through the range of n\n",
        "    for j in range(n): # iterating through the range of n\n",
        "        image = all_images[i * n + j] # getting the image from the list \"all_images\"\n",
        "        stitched_filters[ # setting the stitched filters to the image\n",
        "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width, # setting the cropped width plus the margin multiplied by i to the cropped width plus the margin multiplied by i plus the cropped width\n",
        "            (cropped_height + margin) * j : (cropped_height + margin) * j + cropped_height, # setting the cropped height plus the margin multiplied by j to the cropped height plus the margin multiplied by j plus the cropped height\n",
        "            :, # setting the channel to all the channels\n",
        "        ] = image # setting the stitched filters to the image\n",
        "\n",
        "keras.utils.save_img( # saving the image\n",
        "    f\"filters_for_layer_{layer_name}.png\", stitched_filters) # saving the image as \"filters_for_layer_{layer_name}.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VOTT17HJR"
      },
      "source": [
        "### Visualizing heatmaps of class activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonVDV7z7HJR"
      },
      "source": [
        "**Loading the Xception network with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyzHCmuk7HJS"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(weights=\"imagenet\") # loading the Xception model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeMAZftl7HJS"
      },
      "source": [
        "**Preprocessing an input image for Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnQ9xWgO7HJS"
      },
      "outputs": [],
      "source": [
        "img_path = keras.utils.get_file( # getting the file \"elephant.jpg\" from the url \"https://img-datasets.s3.amazonaws.com/elephant.jpg\"\n",
        "    fname=\"elephant.jpg\", # setting the file name to \"elephant.jpg\"\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\") # setting the url to \"https://img-datasets.s3.amazonaws.com/elephant.jpg\"\n",
        "\n",
        "def get_img_array(img_path, target_size): # defining a function \"get_img_array\" with parameters \"img_path\" and \"target_size\"\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size) # loading the image from the path \"img_path\"\n",
        "    array = keras.utils.img_to_array(img) # converting the image to an array\n",
        "    array = np.expand_dims(array, axis=0) # expanding the dimensions of the array\n",
        "    array = keras.applications.xception.preprocess_input(array) # preprocessing the input using the preprocess input function of the Xception model\n",
        "    return array # returning the array\n",
        "\n",
        "img_array = get_img_array(img_path, target_size=(299, 299)) # getting the image array with the target size of (299, 299) (299 pixels by 299 pixels, because the Xception model was trained on images of this size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auGwAI-M7HJS"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(img_array) # getting the predictions of the model on the image array\n",
        "print(keras.applications.xception.decode_predictions(preds, top=3)[0]) # printing the top 3 predictions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1YsmAxc7HJT"
      },
      "outputs": [],
      "source": [
        "np.argmax(preds[0]) # getting the index of the maximum prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hi74ftd7HJT"
      },
      "source": [
        "**Setting up a model that returns the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSQVvKJS7HJT"
      },
      "outputs": [],
      "source": [
        "last_conv_layer_name = \"block14_sepconv2_act\" # setting the last convolutional layer name to \"block14_sepconv2_act\"\n",
        "classifier_layer_names = [ # creating a list \"classifier_layer_names\"\n",
        "    \"avg_pool\", # setting the first element to \"avg_pool\"\n",
        "    \"predictions\", # setting the second element to \"predictions\"\n",
        "]\n",
        "last_conv_layer = model.get_layer(last_conv_layer_name) # getting the last convolutional layer\n",
        "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output) # creating a model with the inputs as the inputs of the model and the output as the output of the last convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfxtOh4J7HJT"
      },
      "source": [
        "**Reapplying the classifier on top of the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SapRDmWv7HJT"
      },
      "outputs": [],
      "source": [
        "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:]) # creating an input layer for the classifier\n",
        "x = classifier_input # setting x to the input layer\n",
        "for layer_name in classifier_layer_names: # iterating through the classifier layer names\n",
        "    x = model.get_layer(layer_name)(x) # getting the layer with the name \"layer_name\" and applying it to x\n",
        "classifier_model = keras.Model(classifier_input, x) # creating a model with the input as the classifier input and the output as x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETMWcvcD7HJT"
      },
      "source": [
        "**Retrieving the gradients of the top predicted class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6T2MQAE7HJT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the tensorflow module\n",
        "\n",
        "with tf.GradientTape() as tape: # creating a gradient tape\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array) # getting the output of the last convolutional layer on the image array\n",
        "    tape.watch(last_conv_layer_output) # watching the last convolutional layer output\n",
        "    preds = classifier_model(last_conv_layer_output) # getting the predictions of the classifier model on the last convolutional layer output\n",
        "    top_pred_index = tf.argmax(preds[0]) # getting the index of the maximum prediction\n",
        "    top_class_channel = preds[:, top_pred_index] # getting the top class channel\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output) # getting the gradients of the top class channel with respect to the last convolutional layer output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4DUQSjC7HJT"
      },
      "source": [
        "**Gradient pooling and channel-importance weighting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Leq8wNlm7HJU"
      },
      "outputs": [],
      "source": [
        "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy() # getting the pooled gradients by taking the mean of the gradients along the axes (0, 1, 2) and converting it to a numpy array\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0] # getting the last convolutional layer output as a numpy array\n",
        "for i in range(pooled_grads.shape[-1]): # iterating through the range of the last axis of the pooled gradients\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i] # multiplying the last convolutional layer output by the pooled gradients\n",
        "heatmap = np.mean(last_conv_layer_output, axis=-1) # getting the mean of the last convolutional layer output along the last axis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aldBl_1d7HJU"
      },
      "source": [
        "**Heatmap post-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5CasLth7HJU"
      },
      "outputs": [],
      "source": [
        "heatmap = np.maximum(heatmap, 0) # setting the heatmap to the maximum of the heatmap and 0\n",
        "heatmap /= np.max(heatmap) # dividing the heatmap by the maximum of the heatmap\n",
        "plt.matshow(heatmap) # displaying the heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY_sZXYZ7HJU"
      },
      "source": [
        "**Superimposing the heatmap on the original picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9RcjGne7HJU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm # importing the matplotlib.cm module\n",
        "\n",
        "img = keras.utils.load_img(img_path) # loading the image from the path \"img_path\"\n",
        "img = keras.utils.img_to_array(img) # converting the image to an array\n",
        "\n",
        "heatmap = np.uint8(255 * heatmap) # converting the heatmap to an unsigned integer\n",
        "\n",
        "jet = cm.get_cmap(\"jet\") # getting the \"jet\" colormap\n",
        "jet_colors = jet(np.arange(256))[:, :3] # getting the jet colors    \n",
        "jet_heatmap = jet_colors[heatmap] # getting the jet heatmap\n",
        "\n",
        "jet_heatmap = keras.utils.array_to_img(jet_heatmap) # converting the jet heatmap to an image\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0])) # resizing the jet heatmap to the shape of the image\n",
        "jet_heatmap = keras.utils.img_to_array(jet_heatmap) # converting the jet heatmap to an array\n",
        "\n",
        "superimposed_img = jet_heatmap * 0.4 + img # superimposing the jet heatmap on the image\n",
        "superimposed_img = keras.utils.array_to_img(superimposed_img) # converting the superimposed image to an image\n",
        "\n",
        "save_path = \"elephant_cam.jpg\" # setting the save path to \"elephant_cam.jpg\"\n",
        "superimposed_img.save(save_path) # saving the superimposed image to the save path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFNLG7de7HJU"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter09_part03_interpreting-what-convnets-learn.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
