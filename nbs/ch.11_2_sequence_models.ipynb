{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXZm6moGWr43"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnCSroQWr43"
      },
      "source": [
        "### Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48OP3zbiWr44"
      },
      "source": [
        "#### A first practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noaWYZsRWr44"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OkcDhIeeWr44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  1148k      0  0:01:11  0:01:11 --:--:-- 2374k    0  0:03:03  0:00:13  0:02:50  486k   0     0   822k      0  0:01:39  0:00:41  0:00:58 1543k0     0   874k      0  0:01:33  0:00:46  0:00:47 1312k  0   891k      0  0:01:32  0:00:49  0:00:43 1221k  0     0   914k      0  0:01:29  0:00:55  0:00:34 1129k\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # importing the dataset from the stanford website \n",
        "!tar -xf aclImdb_v1.tar.gz # extracting the dataset \n",
        "!rm -r aclImdb/train/unsup # removing the unsupervised data from the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKpwC5HMWr44"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1m5mgJ_Wr45"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random # importing the necessary libraries \n",
        "from tensorflow import keras # importing the keras library from tensorflow\n",
        "batch_size = 32 # setting the batch size to 32\n",
        "base_dir = pathlib.Path(\"aclImdb\") # setting the base directory to aclImdb\n",
        "val_dir = base_dir / \"val\" # setting the validation directory to val\n",
        "train_dir = base_dir / \"train\" # setting the training directory to train\n",
        "for category in (\"neg\", \"pos\"): # for loop to iterate over the categories\n",
        "    os.makedirs(val_dir / category) # making the validation directory\n",
        "    files = os.listdir(train_dir / category) # listing the files in the training directory\n",
        "    random.Random(1337).shuffle(files) # shuffling the files\n",
        "    num_val_samples = int(0.2 * len(files)) # setting the number of validation samples\n",
        "    val_files = files[-num_val_samples:] # setting the validation files\n",
        "    for fname in val_files: # for loop to iterate over the validation files\n",
        "        shutil.move(train_dir / category / fname, \n",
        "                    val_dir / category / fname) # moving the files to the validation directory\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory( # creating the training dataset\n",
        "    \"aclImdb/train\", batch_size=batch_size # setting the batch size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory( # creating the validation dataset\n",
        "    \"aclImdb/val\", batch_size=batch_size # setting the batch size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory( # creating the testing dataset\n",
        "    \"aclImdb/test\", batch_size=batch_size # setting the batch size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) # mapping the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuR5VZtHWr45"
      },
      "source": [
        "**Preparing integer sequence datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrnCY-2yWr46"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers # importing the layers from keras\n",
        "\n",
        "max_length = 600 # setting the maximum length to 600\n",
        "max_tokens = 20000 # setting the maximum tokens to 20000\n",
        "text_vectorization = layers.TextVectorization( # creating the text vectorization layer\n",
        "    max_tokens=max_tokens, # setting the maximum tokens\n",
        "    output_mode=\"int\", # setting the output mode to int, which is integer encoding of the tokens that will be passed to the embedding layer\n",
        "    output_sequence_length=max_length, # setting the output sequence length\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds) # adapting the text vectorization layer\n",
        "\n",
        "int_train_ds = train_ds.map( # mapping the training dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # passing the text vectorization layer to the training dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4\n",
        "int_val_ds = val_ds.map( # mapping the validation dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # passing the text vectorization layer to the validation dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4\n",
        "int_test_ds = test_ds.map( # mapping the testing dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # passing the text vectorization layer to the testing dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pzdFou3Wr46"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFzyuxh2Wr46"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing tensorflow\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer with the shape of None and the data type of int64\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens) # creating the one hot encoding layer with the depth of max_tokens\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # creating the bidirectional LSTM layer with 32 units\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer with a rate of 0.5 \n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the dense layer with 1 unit and the sigmoid activation function\n",
        "model = keras.Model(inputs, outputs) # creating the model with the inputs and outputs \n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the optimizer of rmsprop\n",
        "              loss=\"binary_crossentropy\", # setting the loss function to binary crossentropy\n",
        "              metrics=[\"accuracy\"]) # setting the metrics to accuracy\n",
        "model.summary() # printing the summary of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13B6Cp8lWr46"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtOWY2AUWr46"
      },
      "outputs": [],
      "source": [
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", # setting the model checkpoint to one_hot_bidir_lstm.keras\n",
        "                                    save_best_only=True) # saving the best model only\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model with the training dataset\n",
        "          validation_data=int_val_ds, # setting the validation data to the validation dataset\n",
        "          epochs=10, # setting the epochs to 10\n",
        "          callbacks=callbacks) # setting the callbacks to the callbacks\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\") # loading the model\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lKpQKE0Wr46"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWrs0xxkWr46"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzUQBAbIWr46"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SkxjauNWr46"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256) # creating the embedding layer with the input dimension of max_tokens and the output dimension of 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmnB3zggWr46"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSIfGkHJWr46"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer with the shape of None and the data type of int64\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) # creating the embedding layer with the input dimension of max_tokens and the output dimension of 256\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # creating the bidirectional LSTM layer with 32 units\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer with a rate of 0.5\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the dense layer with 1 unit and the sigmoid activation function\n",
        "model = keras.Model(inputs, outputs) # creating the model with the inputs and outputs\n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the optimizer of rmsprop\n",
        "              loss=\"binary_crossentropy\", # setting the loss function to binary crossentropy\n",
        "              metrics=[\"accuracy\"]) # setting the metrics to accuracy\n",
        "model.summary() # printing the summary of the model\n",
        "\n",
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\", # setting the model checkpoint to embeddings_bidir_gru.keras\n",
        "                                    save_best_only=True) # saving the best model only\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model with the training dataset\n",
        "          validation_data=int_val_ds, # setting the validation data to the validation dataset\n",
        "          epochs=10, # setting the epochs to 10\n",
        "          callbacks=callbacks) # setting the callbacks to the callbacks\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\") # loading the model\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mocSnn2uWr47"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0tMGuDHWr47"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6AP9a0WWr47"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer with the shape of None and the data type of int64\n",
        "embedded = layers.Embedding( # creating the embedding layer\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs) # setting the input dimension to max_tokens, the output dimension to 256, and the mask zero to True\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # creating the bidirectional LSTM layer with 32 units\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer with a rate of 0.5\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the dense layer with 1 unit and the sigmoid activation function\n",
        "model = keras.Model(inputs, outputs) # creating the model with the inputs and outputs\n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the optimizer of rmsprop\n",
        "              loss=\"binary_crossentropy\", # setting the loss function to binary crossentropy\n",
        "              metrics=[\"accuracy\"]) # setting the metrics to accuracy\n",
        "model.summary() # printing the summary of the model\n",
        "\n",
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\", # setting the model checkpoint to embeddings_bidir_gru_with_masking.keras\n",
        "                                    save_best_only=True) # saving the best model only\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model with the training dataset\n",
        "          validation_data=int_val_ds, # setting the validation data to the validation dataset\n",
        "          epochs=10, # setting the epochs to 10\n",
        "          callbacks=callbacks) # setting the callbacks to the callbacks\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\") # loading the model\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiZ0pI6QWr47"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWVmA5jDWr47"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip # downloading the glove embeddings\n",
        "!unzip -q glove.6B.zip # unzipping the glove embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al3BIA0RWr47"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7oePQVYWr47"
      },
      "outputs": [],
      "source": [
        "import numpy as np # importing numpy\n",
        "path_to_glove_file = \"glove.6B.100d.txt\" # setting the path to the glove file\n",
        "\n",
        "embeddings_index = {} # creating the embeddings index dictionary which will contain the word vectors\n",
        "with open(path_to_glove_file) as f: # opening the glove file\n",
        "    for line in f: # iterating over the lines in the file\n",
        "        word, coefs = line.split(maxsplit=1) # splitting the line into the word and the coefs\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # converting the coefs to a numpy array\n",
        "        embeddings_index[word] = coefs # adding the word and the coefs to the embeddings index\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\") # printing the number of word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhW0VYT3Wr47"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ3T1vdyWr47"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100 # setting the embedding dimension to 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary() # getting the vocabulary from the text vectorization layer\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary)))) # creating the word index dictionary\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim)) # creating the embedding matrix with the shape of max_tokens and embedding_dim\n",
        "for word, i in word_index.items(): # iterating over the word index\n",
        "    if i < max_tokens: # if the index is less than the max tokens\n",
        "        embedding_vector = embeddings_index.get(word) # getting the embedding vector\n",
        "    if embedding_vector is not None: # if the embedding vector is not None\n",
        "        embedding_matrix[i] = embedding_vector # adding the embedding vector to the embedding matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agUFadm3Wr47"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding( # creating the embedding layer\n",
        "    max_tokens, # setting the max tokens\n",
        "    embedding_dim, # setting the embedding dimension\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix), # setting the embeddings initializer to the embedding matrix\n",
        "    trainable=False, # setting the trainable to False\n",
        "    mask_zero=True, # setting the mask zero to True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnQovAboWr47"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KCvj8W9Wr47"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer with the shape of None and the data type of int64\n",
        "embedded = embedding_layer(inputs) # creating the embedding layer with the inputs\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded) # creating the bidirectional LSTM layer with 32 units\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer with a rate of 0.5\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the dense layer with 1 unit and the sigmoid activation function\n",
        "model = keras.Model(inputs, outputs) # creating the model with the inputs and outputs\n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model with the optimizer of rmsprop\n",
        "              loss=\"binary_crossentropy\", # setting the loss function to binary crossentropy\n",
        "              metrics=[\"accuracy\"]) # setting the metrics to accuracy\n",
        "model.summary() # printing the summary of the model\n",
        "\n",
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\", # setting the model checkpoint to glove_embeddings_sequence_model.keras\n",
        "                                    save_best_only=True) # saving the best model only\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model with the training dataset\n",
        "            validation_data=int_val_ds, # setting the validation data to the validation dataset\n",
        "            epochs=10, # setting the epochs to 10\n",
        "            callbacks=callbacks) # setting the callbacks to the callbacks\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\") # loading the model\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter11_part02_sequence-models.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
