{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjR1xrIybzNF"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS_4mDOjbzNG"
      },
      "source": [
        "## Neural style transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2XD7GxebzNG"
      },
      "source": [
        "### The content loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRwCIMNxbzNG"
      },
      "source": [
        "### The style loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9G-AcKlbzNG"
      },
      "source": [
        "### Neural style transfer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4s1SohNbzNH"
      },
      "source": [
        "**Getting the style and content images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm6UMsumbzNH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing keras from tensorflow\n",
        "\n",
        "base_image_path = keras.utils.get_file( # getting the base image\n",
        "    \"sf.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/sf.jpg\") # origin of the image\n",
        "style_reference_image_path = keras.utils.get_file( # getting the style reference image\n",
        "    \"starry_night.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/starry_night.jpg\") # origin of the image\n",
        " \n",
        "original_width, original_height = keras.utils.load_img(base_image_path).size # loading the base image and getting the size of the image \n",
        "img_height = 400 # setting the image height to 400\n",
        "img_width = round(original_width * img_height / original_height) # setting the image width to the original width times the image height divided by the original height"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUV7yhA6bzNI"
      },
      "source": [
        "**Auxiliary functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIAtUnrnbzNI"
      },
      "outputs": [],
      "source": [
        "import numpy as np # importing numpy\n",
        "\n",
        "def preprocess_image(image_path): # defining the preprocess image function\n",
        "    img = keras.utils.load_img( # loading the image\n",
        "        image_path, target_size=(img_height, img_width)) # setting the target size of the image\n",
        "    img = keras.utils.img_to_array(img) # converting the image to an array\n",
        "    img = np.expand_dims(img, axis=0) # expanding the dimensions of the image array by adding an axis at the 0th position \n",
        "    img = keras.applications.vgg19.preprocess_input(img) # preprocessing the image\n",
        "    return img # returning the image\n",
        "\n",
        "def deprocess_image(img): # defining the deprocess image function\n",
        "    img = img.reshape((img_height, img_width, 3)) # reshaping the image\n",
        "    img[:, :, 0] += 103.939 # adding 103.939 to the first channel of the image\n",
        "    img[:, :, 1] += 116.779 # adding 116.779 to the second channel of the image\n",
        "    img[:, :, 2] += 123.68 # adding 123.68 to the third channel of the image\n",
        "    img = img[:, :, ::-1] # reversing the order of the channels\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\") # clipping the image and converting it to an unsigned integer\n",
        "    return img # returning the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ28-0AXbzNI"
      },
      "source": [
        "**Using a pretrained VGG19 model to create a feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBW7Ik7IbzNI"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False) # loading the VGG19 model with the imagenet weights and excluding the top layer of the model (the classification layer)\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # creating a dictionary of the model layers and their outputs \n",
        "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict) # creating a model that extracts features from the model layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcLFtwCVbzNI"
      },
      "source": [
        "**Content loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pElMyxGbzNI"
      },
      "outputs": [],
      "source": [
        "def content_loss(base_img, combination_img): # defining the content loss function with the base image and the combination image as parameters\n",
        "    return tf.reduce_sum(tf.square(combination_img - base_img)) # returning the sum of the squared difference between the combination image and the base image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfKj1dPAbzNI"
      },
      "source": [
        "**Style loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T7-O9lqbzNI"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(x): # defining the gram matrix function with x as a parameter\n",
        "    x = tf.transpose(x, (2, 0, 1)) # transposing the x tensor to have the channels first and the height and width second \n",
        "    features = tf.reshape(x, (tf.shape(x)[0], -1)) # reshaping the x tensor to have the first dimension as the number of channels and the second dimension as the product of the height and width\n",
        "    gram = tf.matmul(features, tf.transpose(features)) # multiplying the features tensor by its transpose\n",
        "    return gram # returning the gram matrix\n",
        "\n",
        "def style_loss(style_img, combination_img): # defining the style loss function with the style image and the combination image as parameters\n",
        "    S = gram_matrix(style_img) # calculating the gram matrix of the style image\n",
        "    C = gram_matrix(combination_img) # calculating the gram matrix of the combination image\n",
        "    channels = 3 # setting the number of channels to 3\n",
        "    size = img_height * img_width # setting the size to the product of the image height and width\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # returning the sum of the squared difference between the gram matrices divided by 4 times the square of the number of channels times the square of the size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMkg8pRZbzNJ"
      },
      "source": [
        "**Total variation loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2rz4KV2bzNJ"
      },
      "outputs": [],
      "source": [
        "def total_variation_loss(x): # defining the total variation loss function with x as a parameter\n",
        "    a = tf.square( # calculating the square of the difference between the first and second rows of the x tensor\n",
        "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :] # first row of the x tensor minus the second row of the x tensor \n",
        "    )\n",
        "    b = tf.square( # calculating the square of the difference between the first and second columns of the x tensor\n",
        "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :] # first column of the x tensor minus the second column of the x tensor\n",
        "    )\n",
        "    return tf.reduce_sum(tf.pow(a + b, 1.25)) # returning the sum of the power of the sum of a and b to the 1.25th power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ula_3sVcbzNJ"
      },
      "source": [
        "**Defining the final loss that you'll minimize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1pVBSNabzNJ"
      },
      "outputs": [],
      "source": [
        "style_layer_names = [ # defining the style layer names\n",
        "    \"block1_conv1\", # first convolutional layer in block 1\n",
        "    \"block2_conv1\", # first convolutional layer in block 2\n",
        "    \"block3_conv1\", # first convolutional layer in block 3\n",
        "    \"block4_conv1\", # first convolutional layer in block 4\n",
        "    \"block5_conv1\", # first convolutional layer in block 5\n",
        "]\n",
        "content_layer_name = \"block5_conv2\" # defining the content layer name\n",
        "total_variation_weight = 1e-6 # setting the total variation weight to 1e-6 (0.000001) \n",
        "style_weight = 1e-6 # setting the style weight to 1e-6 (0.000001) \n",
        "content_weight = 2.5e-8 # setting the content weight to 2.5e-8 (0.000000025)\n",
        "\n",
        "def compute_loss(combination_image, base_image, style_reference_image): # defining the compute loss function with the combination image, base image, and style reference image as parameters\n",
        "    input_tensor = tf.concat( # concatenating the base image, style reference image, and combination image along the first axis\n",
        "        [base_image, style_reference_image, combination_image], axis=0 # base image, style reference image, and combination image along the first axis \n",
        "    )\n",
        "    features = feature_extractor(input_tensor) # extracting features from the input tensor\n",
        "    loss = tf.zeros(shape=()) # setting the loss to a tensor with a shape of ()\n",
        "    layer_features = features[content_layer_name] # getting the features of the content layer\n",
        "    base_image_features = layer_features[0, :, :, :] # getting the features of the base image\n",
        "    combination_features = layer_features[2, :, :, :] # getting the features of the combination image\n",
        "    loss = loss + content_weight * content_loss( # adding the content weight times the content loss to the loss\n",
        "        base_image_features, combination_features # base image features and combination features as parameters\n",
        "    )\n",
        "    for layer_name in style_layer_names: # iterating over the style layer names\n",
        "        layer_features = features[layer_name] # getting the features of the layer\n",
        "        style_reference_features = layer_features[1, :, :, :] # getting the features of the style reference image\n",
        "        combination_features = layer_features[2, :, :, :] # getting the features of the combination image\n",
        "        style_loss_value = style_loss( # calculating the style loss value\n",
        "          style_reference_features, combination_features) # style reference features and combination features as parameters\n",
        "        loss += (style_weight / len(style_layer_names)) * style_loss_value # adding the style weight divided by the length of the style layer names times the style loss value to the loss\n",
        " \n",
        "    loss += total_variation_weight * total_variation_loss(combination_image) # adding the total variation weight times the total variation loss to the loss\n",
        "    return loss # returning the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVEp-BgrbzNJ"
      },
      "source": [
        "**Setting up the gradient-descent process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApCJ6Ch9bzNJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing tensorflow\n",
        " \n",
        "@tf.function # defining the compute loss and grads function as a TensorFlow function \n",
        "def compute_loss_and_grads(combination_image, base_image, style_reference_image): # defining the compute loss and grads function with the combination image, base image, and style reference image as parameters\n",
        "    with tf.GradientTape() as tape: # creating a gradient tape\n",
        "        loss = compute_loss(combination_image, base_image, style_reference_image) # calculating the loss\n",
        "    grads = tape.gradient(loss, combination_image) # calculating the gradients\n",
        "    return loss, grads # returning the loss and gradients\n",
        "\n",
        "optimizer = keras.optimizers.SGD( # defining the optimizer as stochastic gradient descent\n",
        "    keras.optimizers.schedules.ExponentialDecay( # using an exponential decay learning rate schedule\n",
        "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96 # initial learning rate of 100.0, decay steps of 100, and decay rate of 0.96\n",
        "    )\n",
        ")\n",
        "\n",
        "base_image = preprocess_image(base_image_path) # preprocessing the base image\n",
        "style_reference_image = preprocess_image(style_reference_image_path) # preprocessing the style reference image\n",
        "combination_image = tf.Variable(preprocess_image(base_image_path)) # creating a TensorFlow variable for the combination image\n",
        "\n",
        "iterations = 4000 # setting the number of iterations to 4000\n",
        "for i in range(1, iterations + 1): # iterating over the range of iterations\n",
        "    loss, grads = compute_loss_and_grads( # calculating the loss and gradients\n",
        "        combination_image, base_image, style_reference_image # combination image, base image, and style reference image as parameters\n",
        "    )\n",
        "    optimizer.apply_gradients([(grads, combination_image)]) # applying the gradients to the combination image\n",
        "    if i % 100 == 0: # if the iteration is divisible by 100\n",
        "        print(f\"Iteration {i}: loss={loss:.2f}\") # print the iteration and loss\n",
        "        img = deprocess_image(combination_image.numpy()) # deprocess the combination image\n",
        "        fname = f\"combination_image_at_iteration_{i}.png\" # setting the file name\n",
        "        keras.utils.save_img(fname, img) # saving the image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEMTBMvtbzNJ"
      },
      "source": [
        "### Wrapping up"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter12_part03_neural-style-transfer.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
