{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZe-ZonYWVMK"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiuF7-6WWVMM"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHcHSGuWVMM"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgrcxHpHWVMM"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7iw_VI5WVMN"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUyA9u_2WVMN"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "booKxL2XWVMN"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEy5dkdoWVMN"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3evUkmKGWVMN"
      },
      "outputs": [],
      "source": [
        "import string # importing the string module to use the punctuation attribute to remove punctuation from the text data to be tokenized and vectorized in the Vectorizer class below \n",
        "\n",
        "class Vectorizer: # defining the Vectorizer class to vectorize text data\n",
        "    def standardize(self, text): # defining the standardize method to convert the text data to lowercase and remove punctuation\n",
        "        text = text.lower() # converting the text data to lowercase\n",
        "        return \"\".join(char for char in text if char not in string.punctuation) # removing punctuation from the text data\n",
        "\n",
        "    def tokenize(self, text): # defining the tokenize method to split the text data into tokens\n",
        "        text = self.standardize(text) # standardizing the text data\n",
        "        return text.split() # splitting the text data into tokens\n",
        "\n",
        "    def make_vocabulary(self, dataset): # defining the make_vocabulary method to create a vocabulary from the text data\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1} # initializing the vocabulary with empty string and unknown token\n",
        "        for text in dataset: # iterating over the text data in the dataset\n",
        "            text = self.standardize(text) # standardizing the text data\n",
        "            tokens = self.tokenize(text) # tokenizing the text data\n",
        "            for token in tokens: # iterating over the tokens in the text data\n",
        "                if token not in self.vocabulary: # checking if the token is not in the vocabulary\n",
        "                    self.vocabulary[token] = len(self.vocabulary) # adding the token to the vocabulary\n",
        "        self.inverse_vocabulary = dict( # creating the inverse vocabulary\n",
        "            (v, k) for k, v in self.vocabulary.items()) # iterating over the vocabulary and creating the inverse vocabulary\n",
        "\n",
        "    def encode(self, text): # defining the encode method to encode text data into integer sequences\n",
        "        text = self.standardize(text) # standardizing the text data\n",
        "        tokens = self.tokenize(text) # tokenizing the text data\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens] # encoding the text data into integer sequences\n",
        "\n",
        "    def decode(self, int_sequence): # defining the decode method to decode integer sequences into text data\n",
        "        return \" \".join( # joining the tokens in the text data\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence) # iterating over the integer sequence and decoding the integer sequence into text data\n",
        "\n",
        "vectorizer = Vectorizer() # instantiating the Vectorizer class\n",
        "dataset = [ # defining the text data to be tokenized and vectorized\n",
        "    \"I write, erase, rewrite\", # first line of text data\n",
        "    \"Erase again, and then\", # second line of text data\n",
        "    \"A poppy blooms.\", # third line of text data\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset) # creating the vocabulary from the text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAShjdauWVMO"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\" # defining the test sentence to be encoded\n",
        "encoded_sentence = vectorizer.encode(test_sentence) # encoding the test sentence\n",
        "print(encoded_sentence) # printing the encoded sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wQqHRF5WVMO"
      },
      "outputs": [],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence) # decoding the encoded sentence\n",
        "print(decoded_sentence) # printing the decoded sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpLPr7NyWVMO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization # importing the TextVectorization class from the tensorflow.keras.layers module to vectorize text data\n",
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    output_mode=\"int\", # setting the output mode to integer\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_Pje3VgWVMO"
      },
      "outputs": [],
      "source": [
        "import re # importing the re module to use the escape function to escape special characters in the punctuation attribute\n",
        "import string # importing the string module to use the punctuation attribute to remove punctuation from the text data to be tokenized and vectorized in the Vectorizer class below\n",
        "import tensorflow as tf # importing the tensorflow module to use the string operations in the TextVectorization class\n",
        "\n",
        "def custom_standardization_fn(string_tensor): # defining the custom_standardization_fn function to standardize the text data\n",
        "    lowercase_string = tf.strings.lower(string_tensor) # converting the text data to lowercase\n",
        "    return tf.strings.regex_replace( # returning the text data with punctuation removed\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\") # removing punctuation from the text data\n",
        " \n",
        "def custom_split_fn(string_tensor): # defining the custom_split_fn function to split the text data into tokens\n",
        "    return tf.strings.split(string_tensor) # splitting the text data into tokens\n",
        "\n",
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    output_mode=\"int\", # setting the output mode to integer\n",
        "    standardize=custom_standardization_fn, # setting the standardize function to the custom_standardization_fn function\n",
        "    split=custom_split_fn, # setting the split function to the custom_split_fn function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DcmVX58WVMO"
      },
      "outputs": [],
      "source": [
        "dataset = [ # defining the text data to be tokenized and vectorized\n",
        "    \"I write, erase, rewrite\", # first line of text data\n",
        "    \"Erase again, and then\", # second line of text data\n",
        "    \"A poppy blooms.\", # third line of text data\n",
        "]\n",
        "text_vectorization.adapt(dataset) # adapting the text data to the TextVectorization class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAosw_MHWVMP"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zVBUm-oWVMP"
      },
      "outputs": [],
      "source": [
        "text_vectorization.get_vocabulary() # getting the vocabulary from the TextVectorization class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAcCVIytWVMP"
      },
      "outputs": [],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary() # getting the vocabulary from the TextVectorization class\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\" # defining the test sentence to be encoded\n",
        "encoded_sentence = text_vectorization(test_sentence) # encoding the test sentence\n",
        "print(encoded_sentence) # printing the encoded sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DD-UVnwWVMP"
      },
      "outputs": [],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary)) # creating the inverse vocabulary\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence) # decoding the encoded sentence\n",
        "print(decoded_sentence) # printing the decoded sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlV-Xl1XWVMP"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1omCQ6WVMP"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8qw6TcyWVMP"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # downloading the IMDB dataset\n",
        "!tar -xf aclImdb_v1.tar.gz # extracting the IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TIvBgNzWVMP"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup # removing the unsupervised training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r0WeyvZWVMP"
      },
      "outputs": [],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt # printing the contents of a positive review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOGoYCudWVMP"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random # importing the os, pathlib, shutil, and random modules to move the validation data to the validation directory\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\") # defining the base directory\n",
        "val_dir = base_dir / \"val\" # defining the validation directory\n",
        "train_dir = base_dir / \"train\" # defining the training directory\n",
        "for category in (\"neg\", \"pos\"): # iterating over the categories in the dataset\n",
        "    os.makedirs(val_dir / category) # creating the validation directory for the category\n",
        "    files = os.listdir(train_dir / category) # listing the files in the training directory for the category\n",
        "    random.Random(1337).shuffle(files) # shuffling the files\n",
        "    num_val_samples = int(0.2 * len(files)) # calculating the number of validation samples\n",
        "    val_files = files[-num_val_samples:] # getting the validation files\n",
        "    for fname in val_files: # iterating over the validation files\n",
        "        shutil.move(train_dir / category / fname, \n",
        "                    val_dir / category / fname) # moving the validation files to the validation directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEr0vPo0WVMQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing the keras module from the tensorflow package to use the text_dataset_from_directory function to create a dataset from a directory\n",
        "batch_size = 32 # defining the batch size\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory( # creating the training dataset\n",
        "    \"aclImdb/train\", batch_size=batch_size # specifying the training directory and batch size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory( # creating the validation dataset\n",
        "    \"aclImdb/val\", batch_size=batch_size # specifying the validation directory and batch size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory( # creating the test dataset\n",
        "    \"aclImdb/test\", batch_size=batch_size # specifying the test directory and batch size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXzzgdQZWVMQ"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_Hy_wRZWVMQ"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds: # iterating over the training dataset\n",
        "    print(\"inputs.shape:\", inputs.shape) # printing the shape of the inputs\n",
        "    print(\"inputs.dtype:\", inputs.dtype) # printing the data type of the inputs\n",
        "    print(\"targets.shape:\", targets.shape) # printing the shape of the targets\n",
        "    print(\"targets.dtype:\", targets.dtype) # printing the data type of the targets\n",
        "    print(\"inputs[0]:\", inputs[0]) # printing the first input\n",
        "    print(\"targets[0]:\", targets[0]) # printing the first target\n",
        "    break # breaking the loop after the first iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVVqTW3WVMQ"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xvKdDHWVMQ"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_fMQrbBWVMQ"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6lSMRMhWVMQ"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    max_tokens=20000, # setting the maximum number of tokens to 20,000\n",
        "    output_mode=\"multi_hot\", # setting the output mode to multi-hot, which returns a binary matrix where each token is represented by a 1 or 0 representing its presence or absence in the text data\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) # mapping the training dataset to only include the text data\n",
        "text_vectorization.adapt(text_only_train_ds) # adapting the text data to the TextVectorization class\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map( # mapping the training dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "binary_1gram_val_ds = val_ds.map( # mapping the validation dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "binary_1gram_test_ds = test_ds.map( # mapping the test dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpCSpcPnWVMQ"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG7xgwulWVMQ"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in binary_1gram_train_ds: # iterating over the training dataset\n",
        "    print(\"inputs.shape:\", inputs.shape) # printing the shape of the inputs\n",
        "    print(\"inputs.dtype:\", inputs.dtype) # printing the data type of the inputs\n",
        "    print(\"targets.shape:\", targets.shape) # printing the shape of the targets\n",
        "    print(\"targets.dtype:\", targets.dtype) # printing the data type of the targets\n",
        "    print(\"inputs[0]:\", inputs[0]) # printing the first input\n",
        "    print(\"targets[0]:\", targets[0]) # printing the first target\n",
        "    break # breaking the loop after the first iteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq4ViUh3WVMR"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU5KQZPjWVMR"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing the keras module from the tensorflow package to use the TextVectorization class\n",
        "from tensorflow.keras import layers # importing the layers module from the tensorflow.keras package to use the Dense and Dropout classes\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16): # defining the get_model function to create a model with a specified number of tokens and hidden dimensions\n",
        "    inputs = keras.Input(shape=(max_tokens,)) # defining the input layer with the specified number of tokens as the shape of the input data \n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs) # defining the hidden layer with the specified number of hidden dimensions and ReLU activation function\n",
        "    x = layers.Dropout(0.5)(x) # adding a dropout layer with a dropout rate of 0.5 to prevent overfitting \n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x) # defining the output layer with a single unit and sigmoid activation function\n",
        "    model = keras.Model(inputs, outputs) # creating the model with the input and output layers\n",
        "    model.compile(optimizer=\"rmsprop\", # compiling the model with the RMSprop optimizer\n",
        "                  loss=\"binary_crossentropy\", # using binary cross-entropy as the loss function\n",
        "                  metrics=[\"accuracy\"]) # using accuracy as the evaluation metric\n",
        "    return model # returning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IryMYwygWVMR"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKzwqiPgWVMR"
      },
      "outputs": [],
      "source": [
        "model = get_model() # creating the model\n",
        "model.summary() # printing the model summary\n",
        "callbacks = [ # defining the callbacks to save the best model\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", # saving the model to the specified file\n",
        "                                    save_best_only=True) # saving only the best model based on the validation loss\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(), # training the model on the training dataset\n",
        "          validation_data=binary_1gram_val_ds.cache(), # validating the model on the validation dataset\n",
        "          epochs=10, # training the model for 10 epochs\n",
        "          callbacks=callbacks) # using the specified callbacks\n",
        "model = keras.models.load_model(\"binary_1gram.keras\") # loading the best model\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\") # evaluating the model on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuIMPtE_WVMR"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKSlJlA4WVMS"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5Qy_YVkWVMS"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    ngrams=2, # using 2-grams to capture more context, which means that each token will represent a pair of words\n",
        "    max_tokens=20000, # setting the maximum number of tokens to 20,000\n",
        "    output_mode=\"multi_hot\", # setting the output mode to multi-hot to represent the presence or absence of each token in the text data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6obRFnxWVMS"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zopTciT1WVMS"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds) # adapting the text data to the TextVectorization class\n",
        "binary_2gram_train_ds = train_ds.map( # mapping the training dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "binary_2gram_val_ds = val_ds.map( # mapping the validation dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "binary_2gram_test_ds = test_ds.map( # mapping the test dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "\n",
        "model = get_model() # creating the model\n",
        "model.summary() # printing the model summary\n",
        "callbacks = [ # defining the callbacks to save the best model\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", # saving the model to the specified file\n",
        "                                    save_best_only=True) # saving only the best model based on the validation loss\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(), # training the model on the training dataset\n",
        "          validation_data=binary_2gram_val_ds.cache(), # validating the model on the validation dataset\n",
        "          epochs=10, # training the model for 10 epochs\n",
        "          callbacks=callbacks) # using the specified callbacks\n",
        "model = keras.models.load_model(\"binary_2gram.keras\") # loading the best model\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\") # evaluating the model on the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqpcVt8_WVMT"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkXF1_mWVMT"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDUisIC9WVMT"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    ngrams=2, # using 2-grams to capture more context, which means that each token will represent a pair of words\n",
        "    max_tokens=20000, # setting the maximum number of tokens to 20,000\n",
        "    output_mode=\"count\" # setting the output mode to count to represent the frequency of each token in the text data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5D45os3WVMT"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFIM3czAWVMT"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization( # instantiating the TextVectorization class\n",
        "    ngrams=2, # using 2-grams to capture more context, which means that each token will represent a pair of words\n",
        "    max_tokens=20000, # setting the maximum number of tokens to 20,000\n",
        "    output_mode=\"tf_idf\", # setting the output mode to TF-IDF to represent the importance of each token in the text data\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NMBIFMNWVMT"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAmK2_MvWVMT"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds) # adapting the text data to the TextVectorization class\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map( # mapping the training dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "tfidf_2gram_val_ds = val_ds.map( # mapping the validation dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "tfidf_2gram_test_ds = test_ds.map( # mapping the test dataset to include the text data and the target data\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text data to the vectorized text data\n",
        "    num_parallel_calls=4) # specifying the number of parallel calls\n",
        "\n",
        "model = get_model() # creating the model\n",
        "model.summary() # printing the model summary\n",
        "callbacks = [ # defining the callbacks to save the best model\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", # saving the model to the specified file\n",
        "                                    save_best_only=True) # saving only the best model based on the validation loss\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(), # training the model on the training dataset \n",
        "          validation_data=tfidf_2gram_val_ds.cache(), # validating the model on the validation dataset\n",
        "          epochs=10, # training the model for 10 epochs\n",
        "          callbacks=callbacks) # using the specified callbacks\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\") # loading the best model\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\") # evaluating the model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDTNuviWWVMT"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\") # defining the input layer with a single string input\n",
        "processed_inputs = text_vectorization(inputs) # processing the input data using the TextVectorization layer\n",
        "outputs = model(processed_inputs) # passing the processed input data through the model\n",
        "inference_model = keras.Model(inputs, outputs) # creating the inference model with the input and output layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKay08h_WVMU"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the tensorflow module to use the string operations in the TextVectorization class\n",
        "raw_text_data = tf.convert_to_tensor([ # defining the raw text data to be classified\n",
        "    [\"That was an excellent movie, I loved it.\"], # positive review\n",
        "])\n",
        "predictions = inference_model(raw_text_data) # making predictions on the raw text data\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\") # printing the percentage of positive sentiment"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter11_part01_introduction.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
