{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qRY1tGcdiy"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COiCSqmicdiz"
      },
      "source": [
        "# Best practices for the real world"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MnpuHmwcdiz"
      },
      "source": [
        "## Getting the most out of your models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfSBMpP6cdi0"
      },
      "source": [
        "### Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSic_IVTcdi0"
      },
      "source": [
        "#### Using KerasTuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm2xqxtecdi0"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner -q # installing keras-tuner library for hyperparameter tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOZCBMgKcdi1"
      },
      "source": [
        "**A KerasTuner model-building function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnyRF0qUcdi1"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing keras library from tensorflow\n",
        "from tensorflow.keras import layers # importing layers from keras\n",
        "\n",
        "def build_model(hp): # defining a function build_model with hp as parameter\n",
        "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16) # defining units as hyperparameter with min_value=16, max_value=64, step=16\n",
        "    model = keras.Sequential([ # defining a sequential model\n",
        "        layers.Dense(units, activation=\"relu\"), # adding a dense layer with units and relu activation function\n",
        "        layers.Dense(10, activation=\"softmax\") # adding a dense layer with 10 units and softmax activation function\n",
        "    ])\n",
        "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"]) # defining optimizer as hyperparameter with values as rmsprop and adam\n",
        "    model.compile( # compiling the model\n",
        "        optimizer=optimizer, # using optimizer as optimizer\n",
        "        loss=\"sparse_categorical_crossentropy\", # using sparse_categorical_crossentropy as loss function\n",
        "        metrics=[\"accuracy\"]) # using accuracy as metric\n",
        "    return model # returning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq2xlasLcdi1"
      },
      "source": [
        "**A KerasTuner `HyperModel`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QMo-bbDcdi1"
      },
      "outputs": [],
      "source": [
        "import kerastuner as kt # importing kerastuner library\n",
        "\n",
        "class SimpleMLP(kt.HyperModel): # defining a class SimpleMLP with kt.HyperModel as parent class\n",
        "    def __init__(self, num_classes): # defining a constructor with self and num_classes as parameters\n",
        "        self.num_classes = num_classes # initializing num_classes with num_classes\n",
        "\n",
        "    def build(self, hp): # defining a function build with self and hp as parameters\n",
        "        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16) # defining units as hyperparameter with min_value=16, max_value=64, step=16\n",
        "        model = keras.Sequential([ # defining a sequential model\n",
        "            layers.Dense(units, activation=\"relu\"), # adding a dense layer with units and relu activation function\n",
        "            layers.Dense(self.num_classes, activation=\"softmax\") # adding a dense layer with num_classes and softmax activation function\n",
        "        ])\n",
        "        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"]) # defining optimizer as hyperparameter with values as rmsprop and adam\n",
        "        model.compile( # compiling the model\n",
        "            optimizer=optimizer, # using optimizer as optimizer\n",
        "            loss=\"sparse_categorical_crossentropy\", # using sparse_categorical_crossentropy as loss function\n",
        "            metrics=[\"accuracy\"]) # using accuracy as metric\n",
        "        return model # returning the model\n",
        "\n",
        "hypermodel = SimpleMLP(num_classes=10) # creating an object hypermodel of class SimpleMLP with num_classes=10 as parameter "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqJpabjbcdi1"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization( # defining a BayesianOptimization tuner\n",
        "    build_model, # using build_model function\n",
        "    objective=\"val_accuracy\", # using val_accuracy as objective\n",
        "    max_trials=100, # using max_trials as 100\n",
        "    executions_per_trial=2, # using executions_per_trial as 2\n",
        "    directory=\"mnist_kt_test\", # using directory as mnist_kt_test\n",
        "    overwrite=True, # using overwrite as True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk1Lqooucdi1"
      },
      "outputs": [],
      "source": [
        "tuner.search_space_summary() # printing the search space summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rwkLNZFcdi2"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data() # loading mnist dataset\n",
        "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255 # reshaping x_train and converting it to float32\n",
        "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255 # reshaping x_test and converting it to float32\n",
        "x_train_full = x_train[:] # copying x_train to x_train_full\n",
        "y_train_full = y_train[:] # copying y_train to y_train_full\n",
        "num_val_samples = 10000 # defining num_val_samples as 10000\n",
        "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:] # splitting x_train into x_train and x_val\n",
        "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:] # splitting y_train into y_train and y_val\n",
        "callbacks = [ # defining a list of callbacks\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5), # adding EarlyStopping callback with monitor as val_loss and patience as 5 \n",
        "]\n",
        "tuner.search( # searching the tuner\n",
        "    x_train, y_train, # using x_train and y_train as parameters \n",
        "    batch_size=128, # using batch_size as 128\n",
        "    epochs=100, # using epochs as 100\n",
        "    validation_data=(x_val, y_val), # using x_val and y_val as validation data\n",
        "    callbacks=callbacks, # using callbacks as callbacks\n",
        "    verbose=2, # using verbose as 2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAnFSQfQcdi2"
      },
      "source": [
        "**Querying the best hyperparameter configurations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEOsl-OQcdi2"
      },
      "outputs": [],
      "source": [
        "top_n = 4 # defining top_n as 4\n",
        "best_hps = tuner.get_best_hyperparameters(top_n) # getting the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYWqMbprcdi2"
      },
      "outputs": [],
      "source": [
        "def get_best_epoch(hp): # defining a function get_best_epoch with hp as parameter\n",
        "    model = build_model(hp) # building the model using hp\n",
        "    callbacks=[ # defining a list of callbacks\n",
        "        keras.callbacks.EarlyStopping( # adding EarlyStopping callback\n",
        "            monitor=\"val_loss\", mode=\"min\", patience=10) # with monitor as val_loss, mode as min and patience as 10\n",
        "    ] \n",
        "    history = model.fit( # fitting the model\n",
        "        x_train, y_train, # using x_train and y_train as parameters\n",
        "        validation_data=(x_val, y_val), # using x_val and y_val as validation data\n",
        "        epochs=100, # using epochs as 100\n",
        "        batch_size=128, # using batch_size as 128\n",
        "        callbacks=callbacks) # using callbacks as callbacks\n",
        "    val_loss_per_epoch = history.history[\"val_loss\"] # getting val_loss per epoch\n",
        "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1 # getting the best epoch\n",
        "    print(f\"Best epoch: {best_epoch}\") # printing the best epoch\n",
        "    return best_epoch # returning the best epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYIjMGS7cdi2"
      },
      "outputs": [],
      "source": [
        "def get_best_trained_model(hp): # defining a function get_best_trained_model with hp as parameter\n",
        "    best_epoch = get_best_epoch(hp) # getting the best epoch\n",
        "    model = build_model(hp) # building the model using hp\n",
        "    model.fit( # fitting the model\n",
        "        x_train_full, y_train_full, # using x_train_full and y_train_full as parameters\n",
        "        batch_size=128, epochs=int(best_epoch * 1.2)) # using batch_size as 128 and epochs as best_epoch * 1.2\n",
        "    return model # returning the model\n",
        "\n",
        "best_models = [] # defining a list best_models\n",
        "for hp in best_hps: # iterating through best_hps \n",
        "    model = get_best_trained_model(hp) # getting the best trained model\n",
        "    model.evaluate(x_test, y_test) # evaluating the model\n",
        "    best_models.append(model) # appending the model to best_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbNEwt1Xcdi2"
      },
      "outputs": [],
      "source": [
        "best_models = tuner.get_best_models(top_n) # getting the best models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9ltlnbfcdi2"
      },
      "source": [
        "#### The art of crafting the right search space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOx46Efocdi2"
      },
      "source": [
        "#### The future of hyperparameter tuning: automated machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm8Tuj-Dcdi3"
      },
      "source": [
        "### Model ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpuv70gicdi3"
      },
      "source": [
        "## Scaling-up model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb_qdGh6cdi3"
      },
      "source": [
        "### Speeding up training on GPU with mixed precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFkPihKycdi3"
      },
      "source": [
        "#### Understanding floating-point precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkSBsi0fcdi3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing tensorflow library\n",
        "import numpy as np # importing numpy library\n",
        "np_array = np.zeros((2, 2)) # creating a numpy array of shape (2, 2)\n",
        "tf_tensor = tf.convert_to_tensor(np_array) # converting numpy array to tensor\n",
        "tf_tensor.dtype # getting the data type of tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfrtribscdi3"
      },
      "outputs": [],
      "source": [
        "np_array = np.zeros((2, 2)) # creating a numpy array of shape (2, 2)\n",
        "tf_tensor = tf.convert_to_tensor(np_array, dtype=\"float32\") # converting numpy array to tensor with data type as float32\n",
        "tf_tensor.dtype # getting the data type of tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqreZt-5cdi3"
      },
      "source": [
        "#### Mixed-precision training in practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nrxt0TCcdi3"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras # importing keras library from tensorflow\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\") # setting the global policy to mixed_float16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OedoOiWhcdi3"
      },
      "source": [
        "### Multi-GPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC-m2m-Mcdi3"
      },
      "source": [
        "#### Getting your hands on two or more GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DioMF-gZcdi3"
      },
      "source": [
        "#### Single-host, multi-device synchronous training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMmdwc_Ecdi3"
      },
      "source": [
        "### TPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ivZ9xMWcdi3"
      },
      "source": [
        "#### Using a TPU via Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2WRHuy6cdi3"
      },
      "source": [
        "#### Leveraging step fusing to improve TPU utilization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au3HY6YDcdi3"
      },
      "source": [
        "## Summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter13_best-practices-for-the-real-world.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
