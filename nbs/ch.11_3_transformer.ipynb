{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTxnE2aHXF91"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZyRcRR_XF92"
      },
      "source": [
        "## The Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqTEZS8JXF92"
      },
      "source": [
        "### Understanding self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS0LLfqDXF92"
      },
      "source": [
        "#### Generalized self-attention: the query-key-value model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h8O2g1dXF93"
      },
      "source": [
        "### Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxjFyNk0XF93"
      },
      "source": [
        "### The Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00iOGswnXF93"
      },
      "source": [
        "**Getting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WWUu8LJXF93"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz # importing the dataset\n",
        "!tar -xf aclImdb_v1.tar.gz # extracting the dataset\n",
        "!rm -r aclImdb/train/unsup # removing the unsupervised data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc-gV2v5XF93"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPeI8OMqXF94"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random # importing the necessary libraries\n",
        "from tensorflow import keras # importing the necessary libraries\n",
        "batch_size = 32 # setting the batch size\n",
        "base_dir = pathlib.Path(\"aclImdb\") # setting the base directory\n",
        "val_dir = base_dir / \"val\" # setting the validation directory\n",
        "train_dir = base_dir / \"train\" # setting the training directory\n",
        "for category in (\"neg\", \"pos\"): # iterating over the categories\n",
        "    os.makedirs(val_dir / category) # creating the validation directory\n",
        "    files = os.listdir(train_dir / category) # listing the files in the training directory\n",
        "    random.Random(1337).shuffle(files) # shuffling the files\n",
        "    num_val_samples = int(0.2 * len(files)) # setting the number of validation samples\n",
        "    val_files = files[-num_val_samples:] # setting the validation files\n",
        "    for fname in val_files: # iterating over the validation files\n",
        "        shutil.move(train_dir / category / fname, \n",
        "                    val_dir / category / fname) # moving the files to the validation directory\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory( # creating the training dataset\n",
        "    \"aclImdb/train\", batch_size=batch_size # setting the batch size and the directory for the training dataset \n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory( # creating the validation dataset\n",
        "    \"aclImdb/val\", batch_size=batch_size # setting the batch size and the directory for the validation dataset\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory( # creating the test dataset\n",
        "    \"aclImdb/test\", batch_size=batch_size # setting the batch size and the directory for the test dataset\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) # extracting the text from the training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6wuJCR2XF94"
      },
      "source": [
        "**Vectorizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRO35f5hXF94"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers # importing the necessary libraries\n",
        " \n",
        "max_length = 600 # setting the maximum length\n",
        "max_tokens = 20000 # setting the maximum tokens\n",
        "text_vectorization = layers.TextVectorization( # creating the text vectorization layer\n",
        "    max_tokens=max_tokens, # setting the maximum tokens\n",
        "    output_mode=\"int\", # setting the output mode\n",
        "    output_sequence_length=max_length, # setting the output sequence length\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds) # adapting the text vectorization layer to the training dataset\n",
        "\n",
        "int_train_ds = train_ds.map( # creating the integer training dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text vectorization layer to the training dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4 (this is the number of CPU cores)\n",
        "int_val_ds = val_ds.map( # creating the integer validation dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text vectorization layer to the validation dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4 (this is the number of CPU cores)\n",
        "int_test_ds = test_ds.map( # creating the integer test dataset\n",
        "    lambda x, y: (text_vectorization(x), y), # mapping the text vectorization layer to the test dataset\n",
        "    num_parallel_calls=4) # setting the number of parallel calls to 4 (this is the number of CPU cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWDiZUJXF94"
      },
      "source": [
        "**Transformer encoder implemented as a subclassed `Layer`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMtkeS2EXF94"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # importing the necessary libraries\n",
        "from tensorflow import keras # importing the necessary libraries\n",
        "from tensorflow.keras import layers # importing the necessary libraries\n",
        "\n",
        "class TransformerEncoder(layers.Layer): # creating the transformer encoder class\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs): # defining the initialization method\n",
        "        super().__init__(**kwargs) # initializing the super class\n",
        "        self.embed_dim = embed_dim # setting the embedding dimension\n",
        "        self.dense_dim = dense_dim # setting the dense dimension\n",
        "        self.num_heads = num_heads # setting the number of heads\n",
        "        self.attention = layers.MultiHeadAttention( # creating the multi-head attention layer\n",
        "            num_heads=num_heads, key_dim=embed_dim) # setting the number of heads and the key dimension\n",
        "        self.dense_proj = keras.Sequential( # creating the dense projection layer\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), # adding a dense layer with the ReLU activation function\n",
        "             layers.Dense(embed_dim),] # adding a dense layer with the embedding dimension\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization() # creating the first layer normalization layer\n",
        "        self.layernorm_2 = layers.LayerNormalization() # creating the second layer normalization layer\n",
        "\n",
        "    def call(self, inputs, mask=None): # defining the call method\n",
        "        if mask is not None: # if the mask is not None\n",
        "            mask = mask[:, tf.newaxis, :] # add a new axis to the mask\n",
        "        attention_output = self.attention( # creating the attention output\n",
        "            inputs, inputs, attention_mask=mask) # setting the inputs and the attention mask\n",
        "        proj_input = self.layernorm_1(inputs + attention_output) # creating the projection input\n",
        "        proj_output = self.dense_proj(proj_input) # creating the projection output\n",
        "        return self.layernorm_2(proj_input + proj_output) # returning the layer normalization\n",
        "\n",
        "    def get_config(self): # defining the get config method\n",
        "        config = super().get_config() # getting the configuration\n",
        "        config.update({ # updating the configuration\n",
        "            \"embed_dim\": self.embed_dim, # setting the embedding dimension\n",
        "            \"num_heads\": self.num_heads, # setting the number of heads\n",
        "            \"dense_dim\": self.dense_dim, # setting the dense dimension\n",
        "        })\n",
        "        return config # returning the configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkbOrazQXF94"
      },
      "source": [
        "**Using the Transformer encoder for text classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAyQYtEbXF95"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000 # setting the vocabulary size\n",
        "embed_dim = 256 # setting the embedding dimension\n",
        "num_heads = 2 # setting the number of heads\n",
        "dense_dim = 32 # setting the dense dimension\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs) # creating the embedding layer\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # creating the transformer encoder layer\n",
        "x = layers.GlobalMaxPooling1D()(x) # creating the global max pooling layer\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the output layer\n",
        "model = keras.Model(inputs, outputs) # creating the model\n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model\n",
        "              loss=\"binary_crossentropy\", # setting the loss function\n",
        "              metrics=[\"accuracy\"]) # setting the metrics\n",
        "model.summary() # printing the model summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTIsHSsIXF95"
      },
      "source": [
        "**Training and evaluating the Transformer encoder based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZMOpxuTXF95"
      },
      "outputs": [],
      "source": [
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", # creating the model checkpoint callback \n",
        "                                    save_best_only=True) # saving only the best model\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model\n",
        "          validation_data=int_val_ds, # setting the validation data\n",
        "          epochs=20, # setting the number of epochs\n",
        "          callbacks=callbacks) # setting the callbacks\n",
        "model = keras.models.load_model( # loading the model\n",
        "    \"transformer_encoder.keras\", # setting the model path\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder}) # setting the custom objects\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulhocAfRXF95"
      },
      "source": [
        "#### Using positional encoding to re-inject order information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2_fzEK6XF95"
      },
      "source": [
        "**Implementing positional embedding as a subclassed layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIioQ-3EXF95"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer): # creating the positional embedding layer\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs): # defining the initialization method\n",
        "        super().__init__(**kwargs) # initializing the super class\n",
        "        self.token_embeddings = layers.Embedding( # creating the token embeddings layer\n",
        "            input_dim=input_dim, output_dim=output_dim) # setting the input and output dimensions\n",
        "        self.position_embeddings = layers.Embedding( # creating the position embeddings layer\n",
        "            input_dim=sequence_length, output_dim=output_dim) # setting the input and output dimensions\n",
        "        self.sequence_length = sequence_length # setting the sequence length\n",
        "        self.input_dim = input_dim # setting the input dimension\n",
        "        self.output_dim = output_dim # setting the output dimension\n",
        "\n",
        "    def call(self, inputs): # defining the call method\n",
        "        length = tf.shape(inputs)[-1] # getting the length\n",
        "        positions = tf.range(start=0, limit=length, delta=1) # getting the positions\n",
        "        embedded_tokens = self.token_embeddings(inputs) # getting the embedded tokens\n",
        "        embedded_positions = self.position_embeddings(positions) # getting the embedded positions\n",
        "        return embedded_tokens + embedded_positions # returning the sum of the embedded tokens and positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None): # defining the compute mask method (this is used for padding)\n",
        "        return tf.math.not_equal(inputs, 0) # returning the not equal inputs to 0 (this is used for padding)\n",
        "\n",
        "    def get_config(self): # defining the get config method\n",
        "        config = super().get_config() # getting the configuration\n",
        "        config.update({ # updating the configuration\n",
        "            \"output_dim\": self.output_dim, # setting the output dimension\n",
        "            \"sequence_length\": self.sequence_length, # setting the sequence length\n",
        "            \"input_dim\": self.input_dim, # setting the input dimension\n",
        "        })\n",
        "        return config # returning the configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4weft1TXF95"
      },
      "source": [
        "#### Putting it all together: A text-classification Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GYHuN_wXF95"
      },
      "source": [
        "**Combining the Transformer encoder with positional embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeBmY-MfXF95"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000 # setting the vocabulary size\n",
        "sequence_length = 600 # setting the sequence length\n",
        "embed_dim = 256 # setting the embedding dimension     \n",
        "num_heads = 2 # setting the number of heads\n",
        "dense_dim = 32 # setting the dense dimension\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # creating the input layer with the integer data type (int64) and None shape (variable length)\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs) # creating the positional embedding layer and passing the inputs to it \n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # creating the transformer encoder layer and passing the positional embedding layer to it\n",
        "x = layers.GlobalMaxPooling1D()(x) # creating the global max pooling layer\n",
        "x = layers.Dropout(0.5)(x) # creating the dropout layer\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # creating the output layer\n",
        "model = keras.Model(inputs, outputs) # creating the model\n",
        "model.compile(optimizer=\"rmsprop\", # compiling the model\n",
        "              loss=\"binary_crossentropy\", # setting the loss function\n",
        "              metrics=[\"accuracy\"]) # setting the metrics\n",
        "model.summary() # printing the model summary\n",
        "\n",
        "callbacks = [ # creating the callbacks\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", # creating the model checkpoint callback \n",
        "                                    save_best_only=True) # saving only the best model\n",
        "]\n",
        "model.fit(int_train_ds, # fitting the model\n",
        "          validation_data=int_val_ds, # setting the validation data\n",
        "          epochs=20, # setting the number of epochs\n",
        "          callbacks=callbacks) # setting the callbacks\n",
        "model = keras.models.load_model( # loading the model\n",
        "    \"full_transformer_encoder.keras\", # setting the model path\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder, # setting the custom objects\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding}) # setting the custom objects\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\") # printing the test accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88zcg7XEXF96"
      },
      "source": [
        "### When to use sequence models over bag-of-words models?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter11_part03_transformer.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
